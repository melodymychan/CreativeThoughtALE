//Reference=MNI152
// Booth et al., 2006 VERBAL VISUAL Semantic decision > phonological (rhyme)		
//Subjects=13		
-39	-57	18
-24	-3	48
-36	42	-21
   		
// Demonet et al., 1992 VERBAL AUDITORY Words > Phonemes		
//Subjects=9		
-48	-55	42
-50	-55	38
-50	-55	33
-20	38	41
-20	37	37
-18	39	32
-1	-60	28
-1	-60	24
-42	-41	-18
		
// Devlin et al., 2003 VERBAL VISUAL Semantic decision > phonol		
//Subjects=12		
-10	52	-8
-14	44	-8
-20	30	48
-42	-66	28
-4	-56	28
8	-82	-28
   		
   		
// Gesierich et al., 2012 nonverbal VISUAL Familiar > unfamiliar/scrambled faces		
//Subjects=21		
-3	-54	12
-12	6	6
-9	-6	6
-3	-36	30
-39	27	6
18	21	-3
-36	33	-12
-39	15	-33
-33	-72	39
-60	-6	-18
60	-3	-15
-3	60	-9
45	-66	30
-21	57	0
-54	-39	-6
18	-45	-9
36	-12	-18
48	6	-27
42	-51	-24
42	-78	-12
45	-53	15
-3	-54	15
-21	-6	-12
-6	-9	3
24	-6	-15
33	-12	-18
-12	9	6
-30	-15	-15
12	12	6
-45	24	21
-42	-72	-18
-42	-81	-15
-36	-75	42
33	33	-12
-39	12	-33
54	-9	-21
-57	-6	-18
6	42	-18
36	12	-33
45	24	21
   		
   		
// Gitelman et al., 2005 VERBAL VISUAL sem>control, inc masked by phono + orth		
//Subjects=14		
-15	-36	66
-12	30	42
-24	12	39
-24	21	36
-51	27	-3
-48	15	9
-54	27	24
-9	15	60
-39	-54	24
-48	12	-15
-42	0	-42
-57	-45	3
48	-15	-18
51	-9	-9
-51	-15	-9
24	-72	-3
15	-87	21
-15	-45	-36
6	-54	-33
27	-78	-42
15	-81	-36
   		
   		
// Gourovitch et al., 2000 VERBAL VISUAL (CUES) Semantic generation > pho		
//Subjects=18		
-1	69	2
-25	-15	-21
-1	35	-17
-8	35	-17
-57	-24	-24
47	-65	15
   		
   		
// Kotz et al., 2002 VERBAL AUDITORY Words > pseudowords		
//Subjects=13		
-24	15	-5
-33	9	5
-14	14	0
-46	-67	23
-56	-44	37
-37	-61	38
54	-50	9
   		
   		
// Mechelli et al., 2007 verbal & nonverbal VISUAL Semantically related > PHONO		
//Subjects=20		
-66	-38	-8
-56	-24	-10
-32	-72	44
-58	-52	40
2	30	40
-6	18	44
-46	24	-14
-52	38	-6
   		
   		
// Mummery et al., 1998 VERBAL VISUAL Semantic decision > phonol		
//Subjects=10		
-48	-72	35
-59	-25	-8
-31	-32	-19
-47	-24	-19
-5	66	18
-14	58	-15
-34	19	-24
   		
   		
// Price et al., 1997 VERBAL VISUAL Semantic decision > phonol		
//Subjects=6		
-34	-5	-26
-52	-69	21
-3	9	4
		
   		
// Raposo et al., 2006 VERBAL VISUAL UNRELATED>IDENTITY		
//Subjects=15		
10	-51	-20
23	-72	-23
29	-64	-26
29	-24	8
40	-39	3
34	-57	14
-42	5	27
-51	24	12
-53	15	17
-3	-32	-4
-10	-43	-11
-33	-38	-2
		
   		
// Roskies et al., 2001 VERBAL VISUAL Semantic decision > phonol		
//Subjects=20		
0	-27	-22
-11	69	-9
-13	55	24
-23	26	36
-26	3	29
-26	-21	-2
-37	26	-31
-39	25	-20
-41	-58	35
-43	44	-17
-52	-7	-19
-5	43	-31
-5	28	-32
-54	23	-8
-53	16	46
-6	-58	17
13	43	11
17	-91	-25
23	20	42
41	-25	-25
58	4	-15
9	-72	-26
   		
   		
// Seghier et al., 2010 verbal & nonverbal VISUAL Semantics > perceptual dec		
//Subjects=94		
-30	-66	42
-48	-68	28
-48	-68	20
-34	-64	24
		
   		
// Sugiura et al., 2006 VERBAL VISUAL Familiarity		
//Subjects=24		
-50	-70	41
54	-72	37
-9	-66	36
12	-64	33
-47	14	-50
50	15	-52
-56	0	-42
-65	-44	37
-34	11	-43
37	15	-42
-60	-6	-39
61	1	-39
-59	-49	35
-41	-81	38
56	-76	36
-5	-68	29
10	-60	30
		
   		
// Wirth et al., 2011 VERBAL VISUAL Semantic > phonological de		
//Subjects=21		
-48	17	-28
-53	-20	-11
-57	-68	21
27	-86	-32
-5	38	53
-2	38	-11
-2	-56	12
				
// Thioux et al., 2005 VERBAL VISUAL animals>numbers (conj of 2 tasks)		
//Subjects=6		
-47	-23	-37
21	-85	-10
		
   		
// Xiao et al., 2005 VERBAL AUDITORY real >pseudowords		
//Subjects=14		
-50	-59	22
55	-47	14
-10	-41	31
		
   		
// Cappa et al., 1998 VERBAL VISUAL words>pseudowords		
//Subjects=13		
-46	27	16
-44	-73	30
-11	28	47
-14	40	37
1	-59	15
47	-76	34
   		
// Craik et al., 1999 VERBAL VISUAL how desirable/accurate the trait is>syllable 		
//Subjects=8		
-8	56	-15
-5	52	31
-38	39	-12
-44	-67	25
-6	56	-15
-5	52	35
-3	22	-16
-40	10	-23
-5	-54	19
-46	-69	25
-3	54	30
-34	26	-16
		
   		
// Baumgaertner et a., 2007 NONVERBAL VISUAL videos > scrambled videos		
//Subjects=19		
48	-69	-3
-48	-72	0
48	15	21
-21	3	57
-48	9	24
-36	33	-15
39	36	-12
-6	-48	6
33	0	54
6	21	51
-51	36	6
		
   		
// Davis et al., 2004 VERBAL VISUAL words> letter strings		
//Subjects=11		
-63	-42	-3
-45	-42	-12
-6	27	-42
-48	30	-15
-33	27	-12
-48	24	18
-18	-9	-15
9	27	42
33	-12	60
		
   		
// Devlin et al., 2002 VERBAL VISUAL any sem category > letter detection		
//Subjects=12		
-28	2	-26
-34	16	-20
-62	-20	-6
42	12	-26
32	6	-50
44	-8	-40
		
   		
// Devlin et al., 2002 VERBAL VISUAL sem categorisation (both animate + inanimate) > letter categorisaion		
//Subjects=8		
-40	-12	-34
-36	-32	-20
-28	12	-30
-48	36	-4
-44	18	26
-6	34	46
18	-84	-32
-58	-38	-2
-64	-44	-2
-46	30	-8
-48	36	4
-48	24	24
48	40	12
48	46	-8
46	30	28
34	22	-8
-2	40	40
-6	18	50
-4	26	46
-20	26	46
-44	40	26
-38	-62	40
-44	-74	40
-46	-62	24
38	-62	40
-6	-56	22
-6	-82	-28
6	-78	-20
-36	-44	-32
-38	-50	-24
-52	-66	-20
-4	-22	8
-18	-6	14
		
   		
// Ebisch et al., 2007 VERBAL & NONVERBAL VISUAL func + visuospatial>control (i.e. real objects/words>not)		
//Subjects=17		
-33	-61	36
-52	28	23
-31	11	55
-5	-67	30
-35	-36	-13
		
   		
// Giraud et al., 2001 VERBAL & NONVERBAL AUDITORY words/meaningful sounds>noise /syllables		
//Subjects=12		
-50	16	35
-57	-9	-21
-72	-45	-15
		
   		
// Grossman et al., 2002a VERBAL VISUAL all nouns>pseudowords		
//Subjects=16		
-55	-72	-6
-21	54	4
-12	-71	7
		
   		
// Grossman et al., 2002b VERBAL VISUAL words>pseudowords		
//Subjects=16		
-68	-49	15
-4	43	-22
1	-96	9
		
   		
// Hagoort et al., 1999 VERBAL VISUAL words>pseudowords		
//Subjects=10		
-18	-50	1
67	-35	18
42	-22	6
-53	-36	-14
60	-48	7
63	-28	-14
14	-16	39
-1	-13	54
		
   		
// Henke et al., 1999 VERBAL VISUAL sem>orth		
//Subjects=12		
-55	23	3
42	-84	-51
72	-7	-32
46	8	-33
58	28	45
-10	20	7
-27	-93	-53
		
   		
// Herbster et al., 1997 VERBAL VISUAL REGULAR>NONWORDS		
//Subjects=10		
-38	-33	-28
		
   		
// Binder et al., 1999 VERBAL AUDITORY semantic>phonology		
//Subjects=30		
-47	-71	38
-7	-56	18
-17	35	46
-27	-38	-18
		
   		
// Joubert et al., 2004 VERBAL VISUAL low frequency >nonwords		
//Subjects=10		
-53	-35	15
-49	-23	12
65	-22	18
61	3	-3
8	-75	24
-50	-67	-32
		
   		
// Kuchinke et al., 2005 VERBAL VISUAL word>nonword		
//Subjects=20		
-36	27	45
-15	57	9
-48	-69	30
-63	-21	-24
-63	-54	-9
-39	-69	42
-21	39	42
-39	-78	36
-12	57	39
-6	-30	33
-6	27	-18
54	-63	18
42	-69	33
12	-57	27
		
   		
// Binder et al., 2003 VERBAL VISUAL word>nonword		
//Subjects=24		
-25	15	55
-16	30	48
-7	49	27
-35	9	47
-31	21	34
0	46	13
-1	47	2
-9	40	-5
1	28	-12
-38	38	-5
-50	-64	26
-38	-74	40
-41	-76	27
-51	-48	43
-50	-65	14
-38	-62	52
-59	-47	-5
-59	-49	-15
-1	-38	35
-9	-60	12
-5	-58	41
-4	-54	27
1	-25	39
-7	-68	56
7	-93	4
-23	-10	-14
-43	-1	24
-55	1	18
-51	-8	43
44	5	33
44	20	29
-1	3	51
0	13	45
27	49	19
-40	17	2
		
   		
// Noppeney et al,. 2003b VERBAL AUDITORY normal>reversed words for all cond (taste, colour & origin)		
//Subjects=9		
-57	-54	-18
-29	-41	-25
-49	28	7
1	-62	8
14	-97	-29
5	-18	-30
-8	73	2
		
   		
// orfanidou et al., 2006 VERBAL AUDITORY words>nonwords		
//Subjects=13		
-38	-36	-16
-38	-70	24
-44	-80	28
-58	-42	46
-32	-82	36
-54	-54	30
-30	-76	46
-36	-68	48
-34	58	16
16	-66	30
14	-66	40
8	-32	46
-4	-14	40
-8	-64	34
12	-56	38
-12	-50	28
42	-72	20
34	-82	36
-22	26	-16
-18	12	-16
14	12	-18
-36	42	-12
54	-50	30
52	-52	20
-56	0	-20
-16	44	10
-6	38	32
-12	50	30
18	20	54
56	-8	10
54	-2	16
50	-8	-14
42	-16	-8
		
   		
// Perani et al., 1999 NONVERBAL VISUAL NON/LIVING OBJECTS>SHAPE RECOGNITION		
//Subjects=11		
-18	-104	-25
-49	34	-25
27	-100	-35
-38	42	-8
-66	-41	10
-59	-49	-8
		
   		
// Perani et al., 1999 VERBAL VISUAL NON/LIVING > PSEUDOWORDS		
//Subjects=8		
4	-7	45
25	-79	3
22	-32	-38
-7	-80	-1
8	-88	0
4	-96	9
4	-9	41
-12	16	-10
-16	-61	15
-24	-37	71
-31	-14	11
53	-10	9
10	-80	-1
38	-24	38
		
   		
// Pilgrim et al., 2002 VERBAL VISUAL words (sem cat) > letter strings (orth cat)		
//Subjects=14		
-50	30	-14
-50	34	-6
-44	36	-10
26	22	-4
36	30	-18
22	22	2
-2	24	48
4	28	44
-2	38	42
-36	-30	-22
-40	-40	-26
-48	-50	-18
-48	-44	0
-62	-40	4
-40	-46	-4
-12	14	2
		
   		
// Rissman et al., 2003 VERBAL AUDITORY words>nonwords		
//Subjects=15		
-65	-46	-7
-1	53	-4
-1	-55	48
-35	-65	41
52	-62	34
		
   		
// Cai et al (2007) Verbal Auditory words>nonwords		
//Subjects=15		
-36	36	-25
-48	29	18
		
   		
// Damasio et al (2001) Non-Verbal Visual Actions (with/out implement) > control task		
//Subjects=10		
-36	30	-11
-46	33	7
-36	12	34
-31	19	-33
-55	-53	-13
-44	-34	-16
58	-35	43
-39	-81	38
-58	-24	30
51	-81	23
-49	-78	24
57	1	3
61	-1	38
72	-29	13
-64	-19	14
-20	-81	42
35	-90	-7
-34	-87	-7
4	-82	15
13	-61	12
-42	0	1
34	-4	1
13	-29	-37
-52	36	13
-36	31	-11
-46	14	27
-25	11	37
-54	-51	-12
-42	-32	-17
58	-40	35
63	-46	31
-36	-82	38
39	40	30
49	2	8
58	3	38
59	-19	10
72	-30	11
-57	-11	3
34	-79	43
-21	-83	40
37	-88	-10
22	-95	-6
-36	-89	-10
9	-78	13
		
   		
// Engelien et al (2006) Non-Verbal Auditory Meaningful sounds > meaningless sounds		
//Subjects=6		
-66	-20	6
-58	24	12
-12	-26	-16
74	24	8
26	40	-28
		
   		
// Rogers et al (2006) Non-Verbal & Verbal Visual ALL SEM/specific >BASELINE		
//Subjects=12		
-42	-74	-16
-32	-90	4
-38	-56	-18
-28	-38	-16
48	-78	-12
38	-88	4
38	-60	-18
26	-28	-20
-54	6	-26
48	22	-28
		
   		
// Tieleman et al (2005) Verbal Visual semantic categorization (animal/object) task > perceptual (small/capital letter string) categorization		
//Subjects=22		
-53	30	5
-38	33	-27
-55	23	18
-46	21	23
0	26	50
4	41	52
-1	41	52
-1	30	38
37	31	-22
35	36	-26
-51	9	-20
-38	24	9
-53	10	10
-53	-42	3
46	16	-27
-27	-17	-20
-25	-25	-15
-27	-32	-20
-19	-14	-21
-38	-47	-17
20	-15	-17
-9	-92	16
-7	-81	8
-38	-47	-18
-3	-71	20
14	-90	13
15	-90	20
17	-68	14
-7	-62	8
-48	21	15
-40	25	-15
-55	24	20
-50	6	43
0	21	58
6	27	40
-7	24	48
38	30	39
51	24	-15
-49	18	-20
-40	15	8
-57	-37	2
-11	-95	11
-22	-93	-3
-20	-94	-2
-42	-52	-13
5	-52	-14
15	-92	23
10	-81	-25
38	-59	-31
		
   		
// Bright et al (2004) Verbal&nonverbal Visual CONJ OF SEM >BASELINE ACROSS 4 TASKS		
//Subjects=38		
-40	32	-21
-51	30	2
-32	19	-42
-34	-41	-22
-36	-17	-36
-32	-27	-31
-14	49	47
-7	57	39
31	-95	-32
25	-89	-34
35	-89	-37
40	26	-21
-36	32	-18
-32	18	-35
-34	-12	-36
-32	-35	-25
		
   		
// Bright et al (2004) verbal Visual WORD ONLY		
//Subjects=38		
-32	10	-30
-23	-37	-23
42	16	-38
		
   		
// Bright et al (2004) Non-Verbal Visual PIC ONLY		
//Subjects=38		
-36	-54	-21
-16	-83	-7
-40	-83	-11
47	-83	-13
44	-56	-22
48	-84	-31
		
   		
// D'Arcy et al (2007) Nonverbal & verbal visual (words + pictures) Sem > baseline		
//Subjects=10		
34	-68	24
-40	-34	-22
-42	14	-38
42	12	-36
-42	-60	-12
44	-70	-8
-42	-68	10
46	-70	12
40	-48	-22
-42	-55	-14
		
   		
// Devlin et al (2000) Verbal Visual sem >baseline (PET)		
//Subjects=16		
-52	34	2
-48	18	30
-42	-14	-28
-36	-32	-16
-28	10	-24
14	-78	-32
		
   		
// Devlin et al (2000) Verbal Visual sem>baseline (FMRI)		
//Subjects=16		
-36	30	-20
-52	36	4
-46	24	-8
52	42	16
46	40	10
-6	20	50
10	-84	-32
		
   		
// Gerlach et al (1999) Non-Verbal Visual Object decision tasks > pattern discrimination		
//Subjects=15		
42	-66	-12
46	-49	-21
38	-90	6
40	-30	-27
33	-2	-45
34	-66	55
-38	-61	-9
-44	-79	-12
-38	-43	-20
-38	-29	-28
-38	-51	-17
-47	-54	-19
-29	-95	-1
-55	-76	-3
-57	-62	-18
-37	-1	-33
49	15	22
55	29	10
		
   		
// Herbster et al (1997) Verbal Visual irregular and regular > say hiya		
//Subjects=10		
-40	-44	-26
-64	-18	-2
-49	1	-8
-12	3	-9
-7	-70	-7
42	-80	-24
		
   		
// Thierry et al (2006) nonverbal auditory aud sounds>scrambled		
//Subjects=12		
-61	-39	7
-62	-40	-11
-60	6	-22
-53	19	21
18	-96	-36
		
		
// Thierry et al (2006) verbal visual visual videos>distorted		
//Subjects=12		
-57	-41	5
-59	-40	-9
-62	10	-27
-61	19	22
25	-93	-34
   		
// Tyler et al (2003) Verbal Visual animals/tools > baseline		
//Subjects=12		
-46	12	32
-50	20	12
-48	28	-10
-30	-34	-26
-20	-14	-14
-42	-42	-26
4	24	36
0	14	48
-6	28	54
0	-80	8
16	-82	18
10	-58	-2
-24	-12	-18
-42	-46	-28
-48	-40	-12
-28	24	-8
-30	28	-18
-44	18	4
-8	-20	20
-6	4	24
-4	-28	16
10	-72	-34
8	-64	-26
34	28	-10
42	18	6
18	26	24
16	-26	-22
10	-20	-20
38	-28	-22
4	20	44
-50	22	10
-32	22	-6
-46	32	-10
6	22	34
-4	14	46
-4	28	44
-44	-40	-24
-20	-12	-24
-18	-10	-14
-24	-58	2
-10	-78	6
-14	-66	2
36	24	-8
44	0	-14
46	20	6
10	-76	-34
6	-60	-22
10	-50	-16
-46	30	-12
-32	32	-18
-50	18	-6
-14	-36	-2
-8	-30	-2
-16	-48	0
-26	-16	-12
-34	-16	-26
-26	-22	-18
-54	-30	-6
-44	-42	-16
-52	-46	-8
-6	-68	-34
6	-36	-26
6	-62	-24
2	18	40
-4	32	40
		
   		
// Grabowski (1998) nonverbal Visual  Naming 		
//Subjects=9		
-43	25	-3
-46	27	3
-29	59	8
		
   		
// Sergent et al (1992) Non-Verbal Visual Identity > gender		
//Subjects=7		
40	20	-43
-38	8	-35
-24	-5	-40
28	-7	-40
27	-16	-26
-3	26	-26
-39	-64	-11
41	-58	-12
-55	-9	-13
24	-62	3
		
   		
// Sergent et al (1992) Non-Verbal Visual Identity > gender		
//Subjects=7		
-58	-42	-18
-39	-62	-13
-56	-9	-15
-42	-80	-3
-32	-56	69
58	-19	32
-56	-16	18
-1	22	-28
		
   		
// Gorno-Tempini et al (1998) Non-Verbal Visual Famous Faces > controls		
//Subjects=6		
28	6	-24
-40	6	-26
-4	44	-14
-2	60	4
-2	-62	30
-8	-56	14
		
   		
// Gorno-Tempini et al (1998) Verbal visual Famous names > controls/nonfamous		
//Subjects=6		
24	4	-26
-42	-2	-24
-2	44	-14
-2	56	6
-16	-62	30
-36	12	-32
-66	-48	6
-56	-62	-18
40	6	-28
-46	-16	-22
-8	-50	-16
4	-48	22
		
   		
// Nakamura et al (2000) Non-Verbal Visual Faces (fam vs unfam task) > control		
//Subjects=7		
34	23	-27
12	-78	-14
-12	-78	-18
15	-92	0
34	-62	-19
-37	-81	-19
29	-6	-29
		
   		
// Leveroni et al (2000) Non-Verbal Visual Familiar faces > unfamiliar		
//Subjects=11		
42	9	-38
-16	36	44
12	58	32
-1	54	-5
57	-5	-23
-55	-16	-16
-52	-63	27
-55	-44	5
49	-26	16
64	-50	18
27	-13	-23
-1	-52	33
10	-46	8
		
   		
// sugiura et al (2001) Non-Verbal Visual Identity discrim (fam or unfam task) > control		
//Subjects=5		
-43	23	-30
48	23	-26
60	1	-16
-45	-74	-18
48	-76	-14
-42	-42	-22
35	-34	-22
-33	-8	-36
-20	-2	-25
26	-1	-21
-10	1	-10
-10	35	-8
-6	50	-11
8	70	12
-2	59	28
4	-53	19
-25	-88	-26
17	-86	-37
-55	-1	-25
		
   		
// Grabowski et al (2001) Non-Verbal Visual naming people/landmarks>baseline (way up) tasks		
//Subjects=10		
-39	14	-28
0	49	16
-39	25	-5
6	-61	19
-21	-30	-25
13	-45	-23
-28	1	34
-1	22	44
-48	-73	36
54	19	-20
49	18	-22
		
   		
// Damasio et al (2004) Non-Verbal Visual pictures>scrambled		
//Subjects=68		
-43	6	-31
-36	26	-8
-2	44	24
-13	47	-2
-10	-41	22
0	-55	21
-2	-34	-19
-50	-52	-15
-29	-33	-21
30	-38	-20
-37	28	17
-37	32	-7
-43	-89	20
-6	-66	-2
11	-72	5
-54	-53	-12
-23	-16	-30
-27	-37	-15
-37	30	-7
-28	15	29
-2	40	27
-10	30	31
-23	-81	34
-19	-73	16
-4	-67	14
		
   		
// Rothstein et al (2005) Non-Verbal Visual Familiarity & identity change effect		
//Subjects=20		
-45	0	-39
63	-6	-33
24	-12	-27
-6	18	69
		
   		
// Elfgren et al (2006) Non-Verbal Visual Familiar > unfamiliar faces during identification		
//Subjects=15		
-13	21	61
-49	35	-3
-51	26	5
12	11	12
-12	41	8
-18	47	35
-46	-52	-13
4	-32	38
-16	-41	39
63	35	0
65	31	-2
52	16	-38
56	16	-27
-64	-43	41
55	-39	3
-14	-63	-3
53	52	-5
-56	-38	0
44	-70	-34
44	-60	-12
-35	-16	-26
		
   		
// Sugiura et al (2008) Verbal visual Familiar > unfamiliar		
//Subjects=25		
-44	10	-32
42	18	-30
-32	12	-22
-32	12	-22
-62	-42	-2
0	-30	36
		
   		
// Nielson et al (2010) Verbal & Nonverbal Visual Familiar > unfamiliar		
//Subjects=17		
-44	14	-29
-10	42	43
0	62	6
-33	13	52
-38	2	-2
-51	-63	28
-1	-51	29
53	-72	35
-57	-43	-6
-55	-17	-13
-47	-3	-38
-26	-31	-13
34	-27	-15
65	-5	-15
69	-52	6
		
   		
// Brambati et al (2010) Non-Verbal Visual Specific judgement of occupation from faces		
//Subjects=12		
64	0	-18
44	-80	-10
-34	-78	-10
38	-78	2
-36	-88	-2
28	-90	26
-14	-102	14
40	-54	-20
36	-51	-21
-36	-50	-22
12	-88	-6
-20	-76	-12
44	18	28
-42	22	20
-62	-8	-10
18	-4	-18
52	-70	0
-58	-38	-4
-40	4	34
-2	18	50
		
   		
// Barense et al (2011) Non-Verbal Visual Familiar faces > unfamiliar faces		
//Subjects=18		
-36	18	-27
-27	3	-21
63	3	-18
-33	-12	-27
-30	-6	-30
-21	-9	-18
-30	-6	-18
27	-15	-18
		
   		
		
   		
// Gesierich et al (2012) Non-Verbal Visual Familiar > Unfamiliar ( with less subjects)		
//Subjects=12		
60	0	-15
-42	15	-33
-6	-66	27
-6	-57	12
-3	-39	30
-42	24	24
-9	-18	27
-33	-72	42
51	-66	27
-57	-6	-18
-24	54	3
15	15	-3
-3	54	-12
-39	27	3
-39	33	-12
-6	9	3
		
   		
// Ross et al (2012) Non-Verbal Visual Famous faces + landmarks > non famous		
//Subjects=16		
-40	21	-28
-4	59	19
-2	60	-5
-5	42	-11
-3	7	-5
-22	-7	-17
-61	-11	-8
-36	-35	-23
5	49	-6
5	50	-7
		
   		
// Dreyer, Felix R.; Pulvermueller, Friedemann 2018 VERBAL VISUAL NOUNS>HASHMARKS		
//Subjects=28		
-42	32	-2
-54	-8	44
-52	14	-10
-56	-18	18
-30	-36	0
-22	0	-4
-42	-52	-14
-32	-18	-10
-24	-10	-10
-6	-16	70
0	-10	64
-60	-38	16
		
    		
// Perrone-Bertolotti, Marcela; Kauffmann, Louise; Pichat, Cedric; Vidal, Juan R.; Baciu, Monica 2017 VERBAL VISUAL WORDS>UNREADABLE FONT		
//Subjects=24		
-6	-3	67
-42	3	25
-48	24	18
-57	-12	49
-57	-42	11
-27	-9	-7
24	-75	-46
		
    		
// Liuzzi, Antonietta Gabriella; Bruffaerts, Rose; Peeters, Ronald; Adamczuk, Katarzyna; Keuleers, Emmanuel; De Deyne, Simon; Storms, Gerrit; Dupont, Patrick; Vandenberghe, Rik 2017 VERBAL aud or visual property verification>heared/saw stimuli?		
//Subjects=18		
-3	-55	7
-12	-58	-11
-9	-46	-2
-30	35	-14
33	35	-14
30	41	-8
-39	26	13
-45	35	13
0	59	-8
-51	-55	-14
-48	-43	-17
-24	-22	-17
-36	-13	-23
-33	-34	-20
		
    		
// Redcay, Elizabeth; Velnoskey, Kayla R.; Rowe, Meredith L. 2016 VERBAL  & NONVERBAL VISUAL MEANINGFUL>MEANINGLESS		
//Subjects=24		
43	-50	14
-60	-47	26
-48	16	-32
-6	-56	41
-8	66	12
14	-53	37
8	59	5
		
    		
// Redcay, Elizabeth; Velnoskey, Kayla R.; Rowe, Meredith L. 2016 NONVERBAL VISUAL COMMUNICATIVE GESTURE>SELF ADAPTOR GESTURE		
//Subjects=24		
-56	-29	6
56	-22	1
-50	-49	13
-53	27	21
55	11	-16
		
    		
// Sheldon, Signy; McAndrews, Mary Pat; Pruessner, Jens; Moscovitch, Morris 2016 VERBAL - INSTRUCTION ONLY VISUAL - INSTRUCTION ONLY SEM FLUENCY> BASELINES		
//Subjects=15		
-30	-42	-12
18	-56	22
30	-40	-12
-36	-78	38
46	-76	26
-26	24	46
30	20	52
36	-56	-26
-18	-10	-14
20	-66	16
-6	20	16
		
    		
// Haberling, Isabelle S.; Corballis, Paul M.; Corballis, Michael C. 2016 NONVERBAL VISUAL pantomimes>(unknown) sign language/videos		
//Subjects=91		
-21	-94	1
21	-94	-2
-6	-55	7
51	-58	7
45	-79	-8
-18	-85	34
-30	-34	-17
39	-34	-17
-9	-55	16
-6	-70	1
27	-16	-14
30	-22	-17
-48	32	7
54	35	4
-27	-10	55
33	-7	55
36	-43	61
-24	-37	70
-6	-4	67
42	-4	58
-24	-49	67
-36	-43	52
60	-28	25
-57	-31	31
-54	-1	-14
54	-61	1
-51	-64	4
-45	-43	-17
54	11	-17
27	-7	-17
-24	-4	-20
-12	-73	-47
18	-76	-47
		
    		
// Haberling, Isabelle S.; Corballis, Paul M.; Corballis, Michael C. 2016 VERBAL VISUAL synonyms>letter strings		
//Subjects=91		
-42	29	-11
39	35	-11
-54	-40	-2
51	-37	1
-51	26	13
57	26	19
-48	17	-23
54	11	-20
-39	8	43
-48	-4	-29
27	-82	-38
-15	-88	-35
-6	23	58
-6	41	43
-27	-13	-17
-9	-94	22
		
    		
// Wang, Xiaojuan; Zhao, Rong; Zevin, Jason D.; Yang, Jianfeng 2016 VERBAL VISUAL WORDS>NONSENSE STROKES		
//Subjects=16		
-35	18	34
-46	4	22
-35	22	1
31	28	2
-1	25	51
-50	-54	-15
-29	-67	34
-54	-40	41
-37	-46	43
		
    		
//  Kumar, Uttam 2016 VERBAL VISUAL CONCRETE & ABSTRACT WORDS>NONWORDS		
//Subjects=20		
-50	11	21
48	19	34
-46	30	30
46	30	21
-38	-84	-2
26	-98	6
-36	-68	-12
-24	-56	44
		
    		
// Higuchi, Hiroki; Moriguchi, Yoshiya; Murakami, Hiroki; Katsunuma, Ruri; Mishima, Kazuo; Uno, Akira 2015 VERBAL VISUAL REAL WORDS>CHECKERBOARD		
//Subjects=28		
42	-66	-10
40	-82	-8
48	-52	-14
-44	-58	-12
-40	-78	-6
-42	-40	-10
		
    		
// AbdulSabur, Nuria Y.; Xu, Yisheng; Liu, Siyuan; Chow, Ho Ming; Baxter, Miranda; Carson, Jessica; Braun, Allen R. 2014 VERBAL VISUAL PRODUCTION & COMPREHENSION>RHYME		
//Subjects=18		
-54	20	15
-48	26	-3
-51	-1	-21
-60	-49	0
-9	14	63
-39	5	51
-45	-64	18
-12	50	33
-3	-52	39
-24	-37	-18
-33	-37	-21
54	2	-21
42	-61	18
9	17	63
18	-79	-45
-30	29	-3
-12	35	48
-24	47	21
-42	20	33
-6	14	54
-12	29	24
-6	-58	45
-3	-61	6
-15	8	15
-3	-16	12
-36	-61	-30
9	-57	12
12	-55	45
42	-76	30
24	-55	0
39	-58	-33
3	-76	-18
-42	2	-42
-57	-4	-9
-33	-46	18
-21	-4	-21
-27	-16	-15
-21	-76	-39
51	26	-3
57	-10	-9
57	-40	-3
60	-58	21
6	44	36
24	-4	-21
		
    		
// Slioussar, Natalia; Kireev, Maxim V.; Chernigovskaya, Tatiana V.; Kataeva, Galina V.; Korotkov, Alexander D.; Medvedev, Svyatoslav V. 2014 VERBAL VISUAL real>nonwords		
//Subjects=21		
-51	-64	28
-30	29	49
-9	-49	37
51	-58	22
-6	-52	31
-48	-64	25
51	-58	25
		
    		
// Bruffaerts, Rose; Dupont, Patrick; Peeters, Ronald; De Deyne, Simon; Storms, Gerrit; Vandenberghe, Rik 2013 VERBAL&NONVERBAL VISUAL pics + words > scrambled pics & consonants		
//Subjects=19		
-36	-43	-23
-36	-22	-23
-9	-58	10
-36	-10	-35
-30	-37	-5
-45	-55	-20
-15	-37	1
-30	-1	-23
-3	-46	25
9	-49	1
27	-16	-17
15	-28	-11
36	-37	-20
6	2	-5
-39	32	10
-30	35	-17
-45	29	-14
-9	17	49
0	17	64
-48	-73	28
39	32	-17
-9	50	46
-12	32	49
		
    		
// Ludersdorfer, Philipp; Schurz, Matthias; Richlan, Fabio; Kronbichler, Martin; Wimmer, Heinz 2013 VERBAL VISUAL words>pseudowords/false fonts		
//Subjects=29		
-9	-97	16
-21	-43	64
15	-94	19
-9	-55	19
15	-49	58
-6	-4	40
51	-31	25
-48	-67	19
-60	-31	16
-51	-58	22
		
    		
// Ludersdorfer, Philipp; Schurz, Matthias; Richlan, Fabio; Kronbichler, Martin; Wimmer, Heinz 2013 VERBAL auditory words>pseudowords/reversed speech		
//Subjects=29		
-45	-58	-11
-48	-64	28
		
    		
// Simard, F.; Monetta, L.; Nagano-Saito, A.; Monchi, O. 2013 VERBAL VISUAL semantic>controls		
//Subjects=14		
-44	-57	-3
-7	43	23
12	43	22
-31	34	-3
40	31	-8
-55	34	24
40	42	17
-51	33	15
-3	28	49
-50	14	44
-1	-32	28
-48	-65	-4
-26	-60	49
32	-65	60
-31	-74	-8
25	-72	-7
-40	-85	-5
34	-90	-1
-7	-88	9
10	-87	13
-5	-12	8
-27	-34	6
25	-28	-2
10	-12	8
-12	1	13
14	10	-4
-40	-68	-29
38	-79	-18
-60	33	-11
-62	36	-3
-57	40	19
-31	-40	-14
-40	-18	-29
-14	-99	5
18	17	-4
-31	28	9
-40	32	-3
-20	-50	-6
-44	-33	-23
6	-74	12
21	-97	20
		
    		
// Straube, Benjamin; Green, Antonia; Weis, Susanne; Kircher, Tilo 2012 NONVERBAL VISUAL iconic>meaningless gesture		
//Subjects=16		
-4	-8	72
-32	28	-8
-56	-40	40
36	8	-28
48	36	-16
-32	-28	-24
-8	-24	0
-8	16	-20
28	-20	-24
56	-36	40
40	12	56
20	60	28
28	-80	-12
		
    		
// Wende, Kim C.; Straube, Benjamin; Stratmann, Mirjam; Sommer, Jens; Kircher, Tilo; Nagels, Arne 2012 VERBAL VISUAL SEM FLUENCY/ASSOCIATION>PHONO FLUENCY		
//Subjects=18		
-38	14	54
-46	-66	30
26	-80	-28
-52	26	10
-54	-10	-26
64	-12	0
30	-22	30
-2	50	-14
-26	-16	28
-54	-20	2
		
    		
// Visser, Maya; Jefferies, Elizabeth; Embleton, Karl V.; Ralph, Matthew A. Lambon 2012 VERBAL&NONVERBAL VISUAL SEMANTIC>CONTROL		
//Subjects=15		
-57	-42	-3
42	21	-33
-54	27	6
-57	-15	-24
		
    		
// Visser, Maya; Jefferies, Elizabeth; Embleton, Karl V.; Ralph, Matthew A. Lambon 2012 NONVERBAL VISUAL PICTURE>CONTROL		
//Subjects=15		
-57	-45	-3
18	-78	-30
		
    		
// Visser, Maya; Jefferies, Elizabeth; Embleton, Karl V.; Ralph, Matthew A. Lambon 2012 VERBAL VISUAL WORD>CONTROL		
//Subjects=15		
-51	-30	-3
-54	33	3
39	21	-33
-45	9	-33
-39	-39	-24
-42	-69	-36
-63	-21	-21  		
    		
// Zhang, John X.; Xiao, Zhuangwei; Weng, Xuchu 2012 VERBAL VISUAL  WORD>NONWORD		
//Subjects=14		
1	51	-12
-20	63	2
-28	42	26
-55	-52	69
66	-49	27
-4	-50	33
		
    		
// Szlachta, Zanna; Bozic, Mirjana; Jelowicka, Aleksandra; Marslen-Wilson, William D. 2012 VERBAL VISUAL  WORDS>MUSICAL RAIN, NOUNS, INFLECTED NOUNS		
//Subjects=21		
-60	-8	-8
-62	-34	4
-42	20	-28
-50	10	-16
-44	32	-8
-46	-40	8
-46	34	6
60	6	-8
62	-16	-2
52	18	-20
56	-26	-2
-30	-34	-24
-48	-62	-22
-38	-46	-20
-60	-8	-2
-62	-32	6
-42	20	-26
-36	34	-16
-44	34	4
58	6	-8
58	-14	-4
48	-32	2
-28	-34	-20
-46	-62	-22
-40	-48	-18
52	-62	24
36	-54	20
-42	-70	24
-62	-6	-6
-62	-36	6
-58	4	-14
-54	10	-18
64	-14	-2
60	2	-8
54	16	-20
46	-32	4
50	18	-24
-34	30	-18
-42	30	-12
-44	34	0
		
    		
// Carota, Francesca; Moseley, Rachel; Pulvermueller, Friedemann 2012 VERBAL VISUAL  WORDS/HASH MARK STRINGS		
//Subjects=18		
-48	42	6
-34	30	-16
-24	32	-16
-46	-8	46
-56	4	36
-58	-12	22
-58	6	20
-60	6	28
-30	-8	54
34	36	-12
60	2	38
34	-8	56
32	-18	54
-56	10	-10
-58	-38	2
-46	-40	-16
-46	-60	-10
-38	-34	-20
-38	-12	-26
-56	-34	40
-48	-30	38
-44	-26	50
-52	-40	26
-64	-20	34
38	-50	-30
8	-54	-6
20	8	2
-48	38	6
52	0	14
44	2	16
-30	30	4
-56	4	16
-30	-8	52
-46	-8	44
-54	-2	44
32	-8	56
-50	-28	4
-56	10	-10
-56	-36	2
-40	-32	-14
-46	-40	-16
-38	-44	-24
38	-48	-32
-26	12	6
-20	4	12
-52	-38	26
-54	-26	32
-50	-32	36
38	-48	-32
-26	32	-12
-46	40	4
36	50	18
-58	-36	0
-64	-40	4
-42	30	20
-46	-24	20
-38	-44	-22
-38	-34	-20
-34	-68	30
-30	-78	36
18	-80	24
-34	30	-16
-34	10	30
-44	10	28
-48	40	6
-50	-10	44
-56	-10	44
-38	-20	42
58	4	40
34	-18	52
-56	-38	2
-42	-12	-26
-44	-60	-10
-46	-40	-14
-38	-34	-20
		
    		
// Geranmayeh, Fatemeh; Brownsett, Sonia L. E.; Leech, Robert; Beckmann, Christian F.; Woodhead, Zoe; Wise, richard J. S. 2012 VERBAL VISUAL SPEECH>TONGUE MOVEMENTS		
//Subjects=19		
-8	22	32
10	18	36
-4	16	52
6	14	56
-50	16	10
-34	14	-6
34	12	-10
28	8	-6
-52	-22	-4
48	-30	-4
		
    		
// Welcome, Suzanne E.; Joanisse, Marc F. 2012 VERBAL VISUAL WORD MEANING>WORD RHYME/WORD CASE		
//Subjects=20		
-57	-36	1
-44	25	24
-2	31	50
42	22	31
36	63	6
-46	-62	50
43	-61	45
		
    		
// Hervais-Adelman, Alexis G.; Carlyon, Robert P.; Johnsrude, Ingrid S.; Davis, Matthew H. 2012 VERBAL AUDITORY INTELLIGIBILITY		
//Subjects=15		
64	-22	0
64	-6	-2
50	-30	2
-60	-18	0
-54	4	-12
-60	-38	4
-38	-44	-14
-38	-24	-16
-18	-6	-12
-30	-2	-18
-24	6	-14
-34	0	38
		
    		
// Hauk, Olaf; Pulvermueller, Friedemann 2011 VERBAL VISUAL READING>HASHMARKS		
//Subjects=21		
-40	-38	-20
-48	-42	-16
-50	30	4
-56	-66	6
-64	-30	2
-62	-38	6
-50	-8	50
-58	-18	22
-38	32	-14
56	-30	2
		  		
// Seghier, Mohamed L.; Josse, Goulven; Leff, Alexander P.; Price, Cathy J. 2011 verbal & nonverbal visual SEM>PERCEPTUAL MATCHING ON MEANINGLESS STIMULI		
//Subjects=60		
-34	32	-14
-42	26	-14
-52	30	-2
-44	48	-8
34	34	-12
-50	16	28
-44	28	16
-54	28	18
-52	14	18
44	20	26
54	22	30
46	32	10
-48	-48	-14
-54	-38	2
-60	-44	-8
-30	-66	42
-48	-68	22
10	-82	-34
28	-74	-44
40	-72	-38
-2	14	52
-2	26	48
-2	36	44
-36	-40	-20
-28	-32	-22
-30	26	4
-36	28	0
32	24	-2
44	24	-6
-14	-14	12
-8	-16	8
		
    		
// Hocking, Julia; McMahon, Katie L.; de Zubicaray, Greig I. 2011 NONVERBAL AUDITORY (+visual instruction) environmental sounds > perceptual baseline		
//Subjects=13		
-6	-51	12
-27	-33	-18
-36	24	-15
-15	33	45
-45	-72	39
39	-69	-39
6	-51	-42
-60	-15	-15
		
    		
// Rapp, Brenda; Lipka, Kate 2011 verbal visual words>checkerboards/consonants		
//Subjects=10		
-39	-47	-9
-36	20	5
-39	1	31
-39	26	22
-36	20	5
24	-18	36
55	-44	-5
-42	-51	5
-24	5	5
58	-6	37
		
    		
// Khader, Patrick H.; Jost, Kerstin; Mertens, Michelle; Bien, Siegfried; Roesler, Frank 2010 VERBAL VISUAL word GENERATION > RHYME GENERATION/letter search		
//Subjects=16		
-66	51	-13
-42	62	3
-53	73	5
-7	63	4
47	66	-3
56	60	-15
58	53	-12
64	31	24
-42	-67	16
-54	-68	18
5	58	12
50	28	-3
2	25	23
8	-58	13
5	59	10
50	25	-1
2	23	22
68	-51	7
55	-77	23
		
    		
// Kuchinke, L; van der Meer, E; Krueger, F 2009 VERBAL VISUAL SEM > GRAMMAR JUDGEMENT		
//Subjects=15		
19	-84	10
-31	2	47
-46	16	25
36	11	45
25	-30	-18
-53	-39	-11
30	-64	51
		
    		
// Cao, F; Peng, D; Liu, L; Jin, Z; Fan, N; Deng, Y; Booth, JR 2009 VERBAL VISUAL SEM>CONTROL (LINE JUDGEMENT)		
//Subjects=13		
27	-90	6
-3	-96	15
-24	-90	-9
-45	27	21
6	-63	-30
-60	-45	0
-3	33	39
-36	-3	-33
30	-69	-36
33	-24	-24
39	30	-3
		
    		
// Leff, AP; Schofield, TM; Stephan, KE; Crinion, JT; Friston, KJ; Price, CJ 2008 verbal auditory normal>reversed speech		
//Subjects=26		
-52	-48	8
-54	10	-16
-48	28	-6
		
    		
// Alain, C; He, Y; Grady, C 2008 verbal auditory CATEGORY>LOCATION JUDGEMENT		
//Subjects=16		
-40	32	7
-1	10	53
-42	-27	10
-56	-4	-3
-56	-34	6
46	-24	9
49	-1	-9
6	-68	-29
		
    		
// Sabri, M; Binder, JR; Desai, R; Medler, DA; Leitl, MD; Liebenthal, E 2008 VERBAL AUDITORY SPEECH>ROTATED/word>pseudoword		
//Subjects=28		
21	-83	34
52	-74	7
19	-50	63
-22	40	-17
-37	21	34
		
    		
// Vingerhoets, G 2008 NONVERBAL VISUAL familiar>unfamiliar tools		
//Subjects=14		
-54	-57	37
-63	-49	26
-7	-69	41
-57	-38	1
		
    		
// Garn, CL; Allen, MD; Larsen, JD 2009 NONVERBAL VISUAL OBJECTS>SCRAMBLED		
//Subjects=26		
31	-79	-15
-24	-92	-6
12	-88	-6
-8	5	54
-40	-85	-15
-30	-69	55
-7	-26	10
1	2	33
		
    		
// Lin, Nan; Wang, Xiaoying; Zhao, Ying; Liu, Yanping; Li, Xingshan; Bi, Yanchao 2015 VERBAL VISUAL WORDS>PSEUDOWORDS		
//Subjects=20		
3	-9	39
60	-30	36
-45	-54	27
		
    		
// Ludersdorfer, Philipp; Wimmer, Heinz; Richlan, Fabio; Schurz, Matthias; Hutzler, Florian; Kronbichler, Martin 2016 VERBAL AUDITORY WORDS>TONES 		
//Subjects=29		
-48	32	7
-42	5	25
-48	20	22
-60	-28	-5
-45	-64	-11
-27	-67	40
-12	-85	34
60	-22	-5
-60	-34	5
-48	20	19
-33	-67	40
-12	-82	28
36	26	-11
60	-22	5
9	-76	7
		
    		
// Segal, Emily; Petrides, Michael 2012 NONVERBAL VISUAL WRITING>COPYING		
//Subjects=90		
-44	16	2
-46	36	8
-12	-18	-6
-12	-34	-6
-28	-67	-12
-50	-48	-14
-24	-98	12
-14	-98	-2
34	18	0
50	36	10
16	-32	2
28	-72	-14
28	-96	18
12	-92	-8
		
    		
// Segal, Emily; Petrides, Michael 2012 VERBAL VISUAL words>nonwordS		
//Subjects=90		
-50	22	-8
-58	-66	26
-66	-52	24
		
    		
// Carota, Francesca; Kriegeskorte, Nikolaus; Nili, Hamed; Pulvermuller, Friedemann 2017 VERBAL VISUAL WORDS>HASHMARKS		
//Subjects=23		
6	20	44
-6	18	48
28	28	-8
-40	-8	-28
-40	-32	-20
-34	7	-30
22	-70	-44
40	-58	-32
38	-48	-32
-52	8	18
-52	0	28
-32	-10	50
		
    		
// Garbin, Gabriele; Collina, Simona; Tabossi, Patrizia 2012 VERBAL VISUAL words>PSEUDOWORDS		
//Subjects=12		
-60	-32	32
-28	32	22
-44	14	16
-62	-20	12
60	-28	16
56	-58	14
-42	18	6
-48	-32	22
-56	-6	16
-58	-4	4
66	-52	10
40	-54	6
		
    		
// Vignali 2019 VERBAL VISUAL WORDS>PSEUDOWORDS		
//Subjects=21		
-45	-73	25
-6	-52	10
9	-55	10
-24	23	49
-9	53	-11
-30	-37	-20
-57	-10	-20
-24	-19	-23
-33	32	-17
30	-31	-20
24	-37	-20
27	-19	-23
27	32	40
-27	-10	7
-15	-1	19
18	5	19
18	14	13
21	-10	19
42	-28	58
48	-13	52
30	-25	58
-9	62	22
		
    		
// Wu, Haiyan; Mai, Xiaoqin; Tang, Honghong; Ge, Yue; Luo, Yue-Jia; Liu, Chao 2013 VERBAL VISUAL WORDS>CHECKERBOARD BASELINE		
//Subjects=19		
-5	14	64
-59	9	44
62	10	47
-49	4	61
59	4	53
-49	-12	67
-25	-60	61
-44	-19	-34
-1	9	75
5	68	31
-59	9	44
62	10	47
-49	4	61
50	-12	71
-28	-52	70
29	-62	71
-52	-43	27
-59	-8	51
59	-9	60
-46	-67	-13
-50	-44	-21
-50	14	-21
-63	0	-15
49	-66	-10
-8	6	81
-59	9	44
62	10	47
-63	27	15
70	14	31
-49	4	61
59	7	53
-28	-54	76
-59	-37	27
59	-16	61
-22	-60	61
-59	8	-4
-50	-44	-21
38	-45	-28
		
    		
// Groussard, M.; Viader, F.; Hubert, V.; Landeau, B.; Abbas, A.; Desgranges, B.; Eustache, F.; Platel, H. 2010 VERBAL VISUAL Verbal semantic>verbal reference		
//Subjects=11		
24	-70	-30
-62	-38	0
		
    		
// Bozic, Mirjana; Marslen-Wilson, William D. 2013 verbal auditory words>musical rain		
//Subjects=13		
-66	-26	-8
-30	-36	-18
-42	-42	-18
-46	-68	28
62	-8	-12
-38	30	-16
-36	36	-10
54	-64	28
4	50	12
-8	36	-10
8	30	-10
34	-38	-16
32	-26	-14
32	-36	-8
		
    		
// Bagga, D.; Singh, N.; Modi, S.; Kumar, P.; Bhattacharya, D.; Garg, M. L.; Khushu, S. 2013 verbal visual sem judgement>matching		
//Subjects=18		
-39	8	25
21	-79	-32
48	29	7
-51	-31	-8
-12	2	64
		
    		
// Chan, ST; Tang, SW; Tang, KW; Lee, WK; Lo, SS; Kwong, KK 2009 verbal visual SYNonyms > PSEUDOCHARACTERS/known language		
//Subjects=22		
-52	24	-2
-6	23	49
-40	-45	49
42	25	-7
-55	-34	-4
-17	10	9
-53	26	0
-1	33	43
-33	-58	49
-34	25	-5
41	26	-5
-57	-40	-10
-49	58	-17
-18	9	8
		
    		
// Malins, Jeffrey G.; Gumkowski, Nina; Buis, Bonnie; Molfese, Peter; Rueckl, Jay G.; Frost, Stephen J.; Pugh, Kenneth R.; Morris, Robin; Mencl, W. Einar 2016 verbal visual unrel>false font/pseudoword		
//Subjects=18		
-49	1	20
0	5	59
-34	27	3
39	27	2
-46	18	18
-50	14	2
-50	-47	-17
-66	-32	6
55	-29	1
-59	-11	-4
-53	33	0
-62	-18	25
62	0	38
23	-63	-20
26	-69	-50
16	-78	-42
-17	8	8
13	11	1
-17	8	8
13	11	1
		
    		
// Harrington, GS; Farias, D; Davis, CH 2009 NONVERBAL VISUAL FAMILIAR>NON-OBJECTS		
//Subjects=8		
-53	-31	41
-18	12	54
-45	3	23
53	14	17
-34	-3	8
-42	29	26
-51	-59	-5
-30	-39	-17
		
    		
// Taylor, MJ; Arsalidou, M; Bayless, SJ; Morris, D; Evans, JW; Barbeau, EJ 2009 NONVERBAL VISUAL known face>unfamiliar		
//Subjects=10		
4	41	14
0	42	14
9	24	27
0	26	26
21	33	26
-15	35	26
-40	41	11
7	39	18
-2	37	19
7	37	24
-1	37	23
8	65	-2
-6	54	-1
-23	44	34
-23	18	-22
-52	-12	-18
-29	17	-10
-26	-2	-15
-20	-26	4
-14	-10	-19
-5	-54	47
7	32	26
0	27	26
8	31	55
		
    		
// Husain, Fatima T.; Patkin, Debra J.; Kim, Jieun; Braun, Allen R.; Horwitz, Barry 2012 nonverbal visual meaningful iconic>meaningless gestures		
//Subjects=16		
-96	-4	26
-100	-2	16
-88	8	12
34	-70	-22
48	-72	-12
46	-56	-22
-42	-76	-18
-50	-72	-16
-38	-62	-22
-24	-50	42
-34	-34	34
-16	-80	50
52	0	22
50	2	34
38	4	16
-42	2	20
-34	-4	30
-32	-14	34
50	-48	52
44	-42	36
56	-36	46
-58	-64	14
		
    		
// Lindenberg, Robert; Uhlig, Marie; Scherfeld, Dag; Schlaug, Gottfried; Seitz, Ruediger J. 2012 nonverbal visual meaningful iconic>meaningless gestures		
//Subjects=20		
-44	22	-23
61	34	-6
-8	64	14
-5	35	29
-54	-12	-18
-4	39	-26
-56	-58	33
11	-89	-22
		
    		
// Mashal, N.; Vishne, T.; Laor, N.; Titone, D. 2013 verbal visual metaphor>unrelated words		
//Subjects=14		
42	26	1
-54	22	22
40	27	2
-28	-90	-6
2	8	62
-3	31	43
-26	2	1
-40	26	7
-45	23	14
-40	25	8
0	7	59
-16	8	5
-11	10	9
		
    		
// Christensen, TA; Antonucci, SM; Lockwood, JL; Kittleson, M; Plante, E 2008 verbal auditory speech>reversed speech		
//Subjects=14		
6	20	43
35	24	3
35	43	20
38	57	0
32	60	-21
32	-43	43
19	5	16
16	-7	10
-5	13	51
-38	21	3
-44	17	24
-34	-37	-22
-46	30	10
-60	-39	12
-11	3	17
-24	16	12
-13	-4	9
5	14	51
35	24	0
35	53	16
47	-34	43
18	1	16
16	-8	13
-5	13	51
-36	24	4
-45	4	32
-37	52	24
-38	-39	-19
		
    		
// Emmorey, Karen; Weisberg, Jill; McCullough, Stephen; Petrich, Jennifer A. F. 2013 verbal visual words>false font		
//Subjects=14		
-4	36	50
7	20	48
37	20	-4
-46	11	34
-53	37	-15
-39	38	-4
-50	36	12
-51	28	1
-54	22	26
-32	35	4
-31	23	-3
-42	-13	-23
-46	-42	-9
-54	-41	4
52	34	26
0	17	49
7	17	49
-49	11	27
-54	46	0
-27	31	4
-39	20	3
-54	-53	-15
-57	-32	50
50	-58	59
-34	-48	43
    		
    		
// Axmacher, N; Bialleck, KA; Weber, B; Helmstaedter, C; Elger, CE; Fell, J 2009 verbal visual words>shapes		
//Subjects=32		
-36	-87	-18
27	-96	-9
-42	-48	-30
		
    		
// Zaccarella, Emiliano; Friederici, Angela D. 2015 verbal visual WORDS>PSEUDOWORDS		
//Subjects=22		
-33	23	-2
36	23	-2
-48	11	7
		
    		
// Jackson, Rebecca L.; Hoffman, Paul; Pobric, Gorana; Ralph, Matthew A. Lambon 2015 verbal visual WORDS>LETTER STRINGS		
//Subjects=24		
60	3	-3
45	-3	-15
27	-48	9
-21	-21	-21
-45	-15	-27
-45	-18	63
-33	-21	72
21	-84	-36
-9	48	39
-9	54	30
-9	57	18
12	-3	45
3	3	39
0	-12	48
-3	54	-12
-15	42	-3
9	-84	27
-3	-84	27
		
    		
// Zou, Lijuan; Packard, Jerome L.; Xia, Zhichao; Liu, Youyi; Shu, Hua 2016 verbal auditory speech > tone		
//Subjects=17		
-45	36	3
33	27	6
-3	18	48
12	-75	-30
-45	48	0
60	-3	-3
-60	-9	3
12	-66	9
-6	-63	9
-5	-81	-24
30	-90	-3
-30	-66	42
-36	-32	-19
0	-51	-15
33	39	-9
-66	-30	3
-6	66	27
0	39	-15
-33	-87	-9
45	27	30
-45	36	3
-60	-12	6
60	-3	-3
-27	-9	-18
-45	-51	-12
60	0	12
-6	21	48
9	-81	-24
12	-84	-36
68	-15	9
0	-48	-36
-66	-30	3
36	39	-6
39	-27	69
-36	-32	-19
-60	-9	6
-39	-24	12
-42	-15	-27
-36	-30	-18
60	-3	-3
48	-15	12
-45	24	24
-51	36	9
-48	48	0
21	-60	-24
-48	-54	-12
-51	-45	-3
-3	15	57
-21	-3	9
-12	-18	9
3	-60	-36
39	42	-9
-27	-51	-30
-30	-39	-24
30	-63	-48
-60	-12	6
-37	-31	-18
-63	-9	0
60	0	0
-51	-15	3
45	-18	6
18	-30	78
-39	30	-12
-60	-5	21
24	-24	75
69	-27	0
		
    		
// Zhang, Hao; Liu, Jia; Zhang, Qinglin 2014 verbal visual nonbiological word relational task> asterisks		
//Subjects=18		
-46	12	31
-3	15	68
62	39	18
13	-101	5
-18	-44	-39
16	-2	-7
23	9	9
-4	18	67
-43	9	31
-43	19	-12
46	0	65
-27	-104	4
20	-94	4
35	-8	-46
42	-14	-31
-24	-27	-5
17	0	23
16	-2	-9
		
    		
// Emmorey, Karen; Xu, Jiang; Gannon, Patrick; Goldin-Meadow, Susan; Braun, Allen 2010 nonverbal visual pantomimes>unknown signs		
//Subjects=14		
-24	-12	66
-48	-48	-24
48	-81	15
-60	-27	48
-25	-51	71
33	-51	69
-63	-27	42
60	-26	43
		
    		
// Smith, Edward E.; Myers, Nicholas; Sethi, Umrao; Pantazatos, Spiro; Yanagihara, Ted; Hirsch, Joy 2012 verbal visual sem>control task		
//Subjects=14		
-32	-36	-18
-10	-56	6
-18	-52	8
-38	-70	26
-38	32	-16
-44	14	44
-50	28	4
-54	-64	14
		
    		
// Ryan, Lee; Lin, Chun-Yu; Ketcham, Katie; Nadel, Lynn 2010 verbal visual semantic judgement > letter/episodic judgement		
//Subjects=15		
-22	-12	-12
-26	-30	-20
-22	-8	-12
26	-34	4
26	-38	-12
-20	-12	-14
-22	-34	-2
-24	-42	-10
-22	-8	-12
26	-34	4
-14	-40	4
-20	-8	-12
-22	-34	0
-20	-6	-12
38	-20	-18
22	-34	4
-22	-12	-14
-18	-6	-24
-22	-8	-12
20	-12	-20
20	-26	-10
20	-26	-12
22	-12	-22
-28	-8	-24
-28	-10	-28
28	-6	-20
		
    		
// Roxbury, Tracy; McMahon, Katie; Copland, David A. 2014 verbal auditory word>pseudoword		
//Subjects=17		
7	-50	47
32	-32	-7
-32	25	54
-54	-14	-25
50	-40	32
14	-50	32
-54	-58	18
		
    		
// Hayashi, Atsuko; Okamoto, Yasumasa; Yoshimura, Shinpei; Yoshino, Atsuo; Toki, Shigeru; Yamashita, Hidehisa; Matsuda, Fumio; Yamawaki, Shigeto 2014 verbal visual word>asterisks		
//Subjects=16		
-6	4	60
14	4	68
-40	36	10
56	22	22
-28	28	-6
36	32	0
-20	6	4
30	6	0
-40	-6	50
-36	-8	66
-22	-62	44
-50	-56	-14
-24	-88	-8
-4	16	52
-46	20	24
56	28	20
-44	-2	48
-46	42	-12
-30	-60	48
-48	-54	-14
-26	-86	-2
38	-82	-4
		
    		
// Sachs, O; Weis, S; Krings, T; Huber, W; Kircher, T 2008 verbal visual relations>letters		
//Subjects=14		
-52	20	28
-60	-36	0
-8	-88	20
-32	-60	36
-48	-4	48
-12	-48	0
16	-100	4
0	-12	68
12	-52	100
-52	20	24
-32	-60	36
0	4	56
20	-100	4
-12	-48	0
-16	-96	-12
-64	-44	0
52	20	28
16	-52	0
-24	-28	-4
40	16	4
		
    		
// Obleser, J; Eisner, F; Kotz, SA 2008 verbal auditory comprehension modulation in vocoded speech		
//Subjects=16		
62	-12	8
-50	-36	-6
-48	4	-22
-62	-16	0
		
    		
// Jensen, Elizabeth J.; Hargreaves, Ian; Bass, Adam; Pexman, Penny; Goodyear, Bradley G.; Federico, Paolo 2011 verbal visual words>nonwords		
//Subjects=12		
-62	-54	-12
-26	34	48
20	42	16
52	-66	20
66	-44	2
4	-34	8
44	-4	56
6	-62	12
30	26	52
18	0	66
30	48	18
60	-42	26
		
    		
// Barros-Loscertales, Alfonso; Gonzalez, Julio; Pulvermueller, Friedemann; Ventura-Campos, Noelia; Carlos Bustamante, Juan; Costumero, Victor; Antonia Parcet, Maria; Avila, Cesar 2012 verbal visual words>hashmarks		
//Subjects=59		
-3	3	60
-9	21	48
-45	-51	-18
-42	-42	-18
-60	-33	3
-51	-6	45
-36	30	9
-36	33	-9
54	0	42
-24	-9	6
-21	-6	-3
		
    		
// Raettig, T; Kotz, SA 2008 verbal auditory words>pseudowords		
//Subjects=16		
-4	-67	36
-49	-70	33
-8	42	-9
67	-14	-21
		
    		
// Moseley, Rachel; Carota, Francesca; Hauk, Olaf; Mohr, Bettina; Pulvermueller, Friedemann 2012 verbal visual words>hashmarks		
//Subjects=18		
-56	4	24
-50	-10	44
-44	26	-2
60	2	38
8	22	52
-8	22	38
6	40	10
-10	54	12
-28	34	-5
-56	-34	2
-38	-44	-14
-40	-62	-12
-48	-34	38
-52	-38	30
8	-62	2
-2	-70	2
-56	4	24
-48	-12	40
-56	-8	44
56	0	40
60	2	24
-10	56	12
8	52	8
-60	-34	2
-40	-40	-14
-44	-74	-8
-48	16	-26
50	-32	24
36	-64	2
16	-50	64
28	24	6
34	-16	22
		
    		
// Weiss, Yael; Katzir, Tami; Bitan, Tali 2015 verbal visual words>asterisks/say pass		
//Subjects=18		
-36	-86	-2
-28	38	-8
-58	-2	20
42	-84	-12
-26	-74	24
62	-10	8
32	38	-8
-2	-60	20
18	-60	-26
-28	-12	-6
-18	26	42
-14	-62	-22
-48	42	0
20	-22	0
28	-56	58
28	-6	-8
-58	-14	24
-28	36	-10
-38	-80	-6
56	-4	24
-14	-64	-20
-2	-60	20
-20	-8	-16
-38	-14	16
-6	48	0
18	-64	-24
-46	28	10
-4	22	32
-8	-12	14
-46	-72	32
		
    		
// Chouinard, PA; Morrissey, BF; Kohler, S; Goodale, MA 2008 nonverbal visual naming intact objects > counting scrambled objects		
//Subjects=14		
-46	14	28
-4	20	34
-2	12	54
-38	-42	-18
-54	-58	-18
-52	-62	-12
-44	-80	-8
34	-68	-20
30	-74	-4
32	-86	0
		
    		
// Zvyagintsev, Mikhail; Clemens, Benjamin; Chechko, Natalya; Mathiak, Krystyna A.; Sack, Alexander T.; Mathiak, Klaus 2013 none none visual imagery>counting		
//Subjects=15		
52	34.3	-12
35	-81	-38
-26	-31	-21
23	-13	3
-52	43	-12
-4	56	-13
-42	-66	26
		
    		
// Bick, A; Goelman, G; Frost, R 2008 verbal visual SEM TASK>VISUAL CONTROL/words>pseudowords		
//Subjects=14		
-18	-93	-2
-52	-37	3
-37	10	33
-46	28	15
-2	55	37
-17	-91	-2
-40	-65	-11
-33	-60	40
10	-73	25
-53	-34	1
-44	6	52
-44	24	21
-15	10	-2
-17	-91	-2
-42	-63	-11
6	-72	24
-32	-65	44
-53	-31	-1
2	-28	32
-2	-11	10
-45	26	20
-18	-92	-3
-33	-64	43
-45	-55	-12
-24	-31	-1
-48	1	50
-44	23	21
		
    		
// Raposo, Ana; Frade, Sofia; Alves, Mara 2016 verbal visual semantic>perceptual task		
//Subjects=18		
-4	-54	16
-50	28	-10
-8	52	36
-8	-84	-6
14	-96	16
-46	-68	26
32	34	-12
50	16	-28
		
    		
// Heim, S; Eickhoff, SB; Amunts, K 2008 none/verbal instruction prior to task none (visual instruction prior) sem>phono fluency only		
//Subjects=28		
-60	-10	-23
-28	-38	-19
		
    		
// Gutchess, Angela H.; Hedden, Trey; Ketay, Sarah; Aron, Arthur; Gabrieli, John D. E. 2010 verbal visual sem relationships>same words		
//Subjects=20		
-46	-52	-16
-4	-26	-20
-46	26	22
-18	-4	20
-18	0	10
-30	-62	50
-48	12	24
38	24	-4
4	-70	-32
-28	-32	0
		
    		
// Marques, JF; Canessa, N; Siri, S; Catricala, E; Cappa, S 2008 verbal visual CONJUNCTION OF VISUAL AND MOTION FEATURES >CONTROL task		
//Subjects=21		
-28	-90	0
32	-88	2
-56	-48	-2
0	8	60
-4	-2	60
-52	12	16
-36	28	0
-42	32	-2
		
    		
// Menz, Mareike M.; Blangero, Annabelle; Kunze, Damaris; Binkofski, Ferdinand 2010 nonverbal (except instructions) visual known>unknown objects		
//Subjects=20		
-56	-29	76
-26	28	44
-8	34	-15
-11	49	38
		
    		
// Ryan, L; Cox, C; Hayes, SM; Nadel, L 2008 verbal (cue only) visual (cue only) semantic fluency>string of xs		
//Subjects=10		
-23	-19	-16
-19	-26	-17
21	-18	-19
-4	34	22
-29	30	-10
-26	16	56
-16	-1	9
-6	-22	-20
-4	-50	3
7	-47	3
-30	-73	-21
21	-80	-18
33	26	1
-45	8	47
14	7	1
-5	-10	3
		
    		
// Grindrod, Christopher M.; Garnett, Emily O.; Malyutina, Svetlana; den Ouden, Dirk B. 2014 verbal visual  words>nonwords		
//Subjects=23		
45	14	1
-63	-49	7
-39	11	-2
48	-52	7
15	14	58
-30	47	22
0	-37	49
3	23	25
-33	50	-14
-12	44	40
-21	-94	19
		
    		
// Chiao, JY; Harada, T; Oby, ER; Li, Z; Parrish, T; Bridge, DJ 2009 nonverbal visual status>colour change detection		
//Subjects=12		
33	-62	42
56	-53	-6
-48	7	33
-3	14	52
-50	-35	49
36	23	-4
-33	20	-1
-30	-69	-44
-42	-67	1
50	14	38
33	2	50
0	-33	32
45	-42	-15
33	-62	42
-3	17	49
-30	-56	53
36	23	-4
-36	20	-1
53	24	24
3	-33	29
0	31	37
36	23	-7
33	-62	42
33	-59	53
-36	20	-1
36	-39	-17
-9	-77	-18
-36	-50	-14
56	25	29
-30	-61	56
-45	52	0
-33	-50	47
		
    		
// Jeon, HA; Lee, KM; Kim, YB; Cho, ZH 2009 verbal visual word generation/reading>nonword reading		
//Subjects=16		
-42	22	22
-28	-62	51
-54	30	21
-42	22	18
-42	22	22
-44	22	-3
-28	-56	63
-33	-53	54
		
    		
// Sun, Kaibao; Xue, Rong; Zhang, Peng; Zuo, Zhentao; Chen, Zhongwei; Wang, Bo; Martin, Thomas; Wang, Yi; Chen, Lin; He, Sheng; Wang, Danny J. J. 2017 verbal visual semantic>orthography		
//Subjects=11		
-58	-44	-8
-64	-40	-10
-60	-30	-14
		
    		
// Liu, L; Deng, XX; Peng, DL; Cao, F; Ding, GS; Jin, Z; Zeng, YW; Li, K; Zhu, L; Fan, N; Deng, Y; Bolger, DJ; Booth, JR 2009 verbal visual words>slashes 		
//Subjects=16		
-52	34	20
-48	26	20
-50	16	40
-36	-48	-24
-28	-100	-8
-4	20	48
-36	-12	-36
-38	-58	-24
28	-92	-16
26	-96	-8
36	-64	-24
34	24	0
-46	32	16
-44	22	8
-30	-64	36
-38	-44	-28
-36	-62	-28
36	20	-8
40	20	8
28	-96	-4
8	-50	-32
38	-58	-28
18	-30	20
18	6	20
		
    		
// Liu, L; Deng, XX; Peng, DL; Cao, F; Ding, GS; Jin, Z; Zeng, YW; Li, K; Zhu, L; Fan, N; Deng, Y; Bolger, DJ; Booth, JR 2009 verbal auditory words>tones		
//Subjects=16		
-30	22	-8
-2	-84	0
-50	-4	-12
-60	-24	0
-10	0	8
-8	-18	12
32	24	-4
50	34	28
2	24	44
6	32	40
60	-12	-8
50	12	-16
-46	32	20
-46	10	20
-52	-2	12
-62	-24	4
-48	4	44
2	20	52
42	14	-28
0	-78	-20
		
    		
// Diaz, MT; McCarthy, G 2009 verbal visual words>nonwords		
//Subjects=16		
-54	-60	36
-66	-16	-12
66	-10	-16
74	-26	-5
66	-38	-12
-40	12	-42
-47	34	-14
34	36	-20
24	-16	-24
-11	52	23
-10	36	56
		
    		
// Taminato, Tomoya; Miura, Naoki; Sugiura, Motoaki; Kawashima, Ryuta 2014 nonverbal visual object recog>control task 		
//Subjects=35		
-34	-86	18
42	-78	14
-16	-70	-6
-46	-66	-14
44	-56	-16
-34	-50	-16
-32	-36	-24
34	-52	-10
32	-38	-20
32	-48	-12
-40	10	24
-32	-88	16
40	-82	20
-8	-80	-10
-46	-54	-16
40	-64	-14
-32	-64	-12
32	-64	-12
32	-42	-22
-38	12	28
-6	20	44
		
    		
// Wright, ND; Mechelli, A; Noppeney, U; Veltman, DJ; Rombouts, SARB; Glensman, J; Haynes, JD; Price, CJ 2008 verbal & nonverbal visual  semantic>visual matching		
//Subjects=34		
-42	-56	-14
-50	-58	-12
		
    		
// Wright, ND; Mechelli, A; Noppeney, U; Veltman, DJ; Rombouts, SARB; Glensman, J; Haynes, JD; Price, CJ 2008 verbal visual  semantic>visual matching		
//Subjects=34		
-48	-58	-14
-52	-52	-10
		
    		
// Wright, ND; Mechelli, A; Noppeney, U; Veltman, DJ; Rombouts, SARB; Glensman, J; Haynes, JD; Price, CJ 2008 nonverbal visual  semantic>visual matching		
//Subjects=34		
-46	-52	-10
-54	-58	-10
		
    		
// Leung, Ada W. S.; Alain, Claude 2010 nonverbal auditory category>location matching		
//Subjects=16		
-5	12	57
-47	28	-16
-27	24	11
-51	-41	7
51	25	17
12	-87	-30
		
    		
// Soch, Joram; Deserno, Lorenz; Assmann, Anne; Barman, Adriana; Walter, Henrik; Richardson-Klavehn, Alan; Schott, Bjoern H. 2017 verbal visual self/other attribute judgement>syllable counting		
//Subjects=110		
-9	50	37
-42	23	-14
-48	-67	25
-3	-52	28
54	-64	28
-54	-7	-23
50	-7	-20
0	11	-11
0	-19	37
		
    		
// Chou, TL; Chen, CW; Wu, MY; Booth, JR 2009 verbal visual sem semantic >perceptual task		
//Subjects=31		
-48	30	-3
-47	24	12
-6	36	39
-57	-46	-3
-48	30	-3
-45	24	14
-60	-45	0
		
    		
// Wright, Paul; Randall, Billi; Marslen-Wilson, William D.; Tyler, Lorraine K. 2011 verbal auditory speech>musical rain		
//Subjects=14		
-60	-3	-6
-60	-30	3
-63	-18	-3
63	-18	-3
60	-6	-3
54	-27	3
		
// Smirnov, Dmitry; Glerean, Enrico; Lahnakoski, Juha M.; Salmi, Juha; Jaaskelainen, Iiro P.; Sams, Mikko; Nummenmaa, Lauri, 2014, VERBAL&NONVERBAL, VISUAL, MISCUE>CUE		
//Subjects=20		
-43	18	18
		
// Willems, RM; Ozyurek, A; Hagoort, P, 2008, VERBAL&NONVERBAL, VISUAL, incongruent>congruent SEQUENCES		
//Subjects=19		
-45	22	20
-53	-30	-7
-40	-61	18
19	-42	-31
		
// Krieger-Redwood, Katya; Teige, Catarina; Davey, James; Hymers, Mark; Jefferies, Elizabeth, 2015, auditory, verbal, strong>weak associations between words		
//Subjects=22		
-4	28	32
-4	26	36
0	18	50
4	24	52
0	30	46
-6	28	22
2	-60	50
16	-62	24
-8	-68	34
-4	-70	34
-6	-64	50
-6	-58	48
-34	22	-10
-46	14	-10
-14	0	-4
-52	20	-12
-48	20	2
-16	18	-6
-32	48	6
-22	48	4
-26	62	-10
-28	62	-6
-28	58	2
-16	58	-14
32	20	-4
40	18	-12
30	16	-18
42	20	-6
22	8	-24
22	8	-20
54	24	18
54	22	26
48	26	30
60	24	12
36	20	16
		
// Balthasar, Andrea J. R.; Huber, Walter; Weis, Susanne, 2011, auditory, verbal, homonyms>identification		
//Subjects=18		
27	-61	-29
-20	67	15
1	-27	4
-12	16	-18
56	11	6
1	-91	-6
27	48	-4
1	-32	4
24	-39	66
-12	8	-13
23	-75	-12
23	-26	37
-12	-57	-30
10	50	13
1	-5	5
35	20	-23
32	-80	12
-50	-79	16
49	23	25
1	-25	-12
-50	-56	38
5	58	12
-4	30	-21
35	29	-20
-43	20	-26
49	-79	19
-38	-38	14
11	-55	66
27	-39	-26
-46	-41	-12
-3	-32	29
32	-52	31
5	-81	-22
		
// Hsu, Nina S.; Kraemer, David J. M.; Oliver, Robyn T.; Schlichting, Margaret L.; Thompson-Schill, Sharon L., 2011, auditory, verbal, less>more distance to foil		
//Subjects=12		
-2	18	41
-40	5	49
-72	-26	6
-35	18	-1
-18	-26	-3
11	-71	15
-18	-86	-2
47	-26	-2
33	20	-8
-40	-36	50
-47	21	21
65	15	26
-44	-56	-12
		
// Rodd, Jennifer M.; Longe, Olivia A.; Randall, Billi; Tyler, Lorraine K. 2010, auditory, visual, SEMANTIC AMBIGUITY IN SENTENCES (HOMONYMS/PSEUDOHOMONYMS)>not		
//Subjects=14		
-52	10	12
		
// Rodd, Jennifer M.; Longe, Olivia A.; Randall, Billi; Tyler, Lorraine K. 2010, auditory, visual, SEMANTIC AMBIGUITY IN SENTENCES (HOMONYMS/PSEUDOHOMONYMS)>syntactic		
//Subjects=14		
-52	22	4
		
// Vitello, Sylvia; Warren, Jane E.; Devlin, Joseph T.; Rodd, Jennifer M., 2014, auditory, verbal, ambiguous word in sentence>not		
//Subjects=20		
-45	32	4
-45	-55	-11
-48	-58	-8
		
// Rodd, Jennifer M.; Johnsrude, Ingrid S.; Davis, Matthew H. 2012, auditory, verbal, ambiguity across time		
//Subjects=15		
-52	22	18
-52	36	2
-44	20	26
-44	12	32
-48	38	-10
-48	20	-8
-44	-50	-18
-60	-48	8
-48	-56	-12
-54	-44	-8
-52	-38	-2
-56	-58	0
-50	-44	6
-62	-50	10
-32	-38	-18
-56	-26	-6
		
// Tahmasebi, Amir M.; Davis, Matthew H.; Wild, Conor J.; Rodd, Jennifer M.; Hakyemez, Helene; Abolmaesumi, P.; Johnsrude, Ingrid S., 2012, auditory, verbal, ambiguous homonym>unambiguous word		
//Subjects=24		
-49	-67	-4
-53	10	25
		
// Bekinschtein, Tristan A.; Davis, Matthew H.; Rodd, Jennifer M.; Owen, Adrian M., 2011, auditory, verbal, ambiguous homonym/homophone>unambiguous word		
//Subjects=12		
-42	-62	-16
-44	-48	-20
		
// Tune, Sarah; Schlesewsky, Matthias; Nagels, Arne; Small, Steven L.; Bornkessel-Schlesewsky, Ina, 2016, auditory, verbal, anomalous>normal sentences		
//Subjects=18		
-61	-32	26
-50	-15	2
-47	-43	-18
-7	-82	31
-4	-16	33
-22	-37	1
17	54	23
58	-29	35
33	5	-16
47	-58	38
27	15	43
65	-22	1
-49	-45	45
-7	40	4
-2	-9	12
-59	-34	39
63	-47	-2
18	-4	-14
51	33	-1
2	-7	9
63	-32	-14
		
// Willems, Roel M.; Frank, Stefan L.; Nijhof, Annabel D.; Hagoort, Peter; van den Bosch, Antal 2016, auditory, verbal, word context surrpisal		
//Subjects=24		
-46	-46	-16
-64	-36	12
-58	-50	10
-54	-58	6
-52	0	-4
-56	8	-6
56	8	-6
30	-2	-11
12	-12	-8
32	-6	-12
50	12	-4
64	-26	10
		
// Obleser, Jonas; Kotz, Sonja A., 2010, auditory, verbal, low>high cloze probability		
//Subjects=16		
-60	12	16
-50	-42	2
40	-32	8
-60	14	14
-52	-8	-16
46	-18	-6
		
// Scharinger, Mathias; Bendixen, Alexandra; Herrmann, Bjorn; Henry, Molly J.; Mildner, Toralf; Obleser, Jonas, 2016, auditory, verbal, unpredictable>predicatable word when incomplete>complete sentence		
//Subjects=22		
60	-4	-2
-57	-16	-2
-42	-49	49
-42	-49	34
		
// Rothermich, Kathrin; Kotz, Sonja A. 2013, auditory, verbal, semantic unpredicatable>predictable		
//Subjects=16		
-54	29	16
-54	29	16
-54	29	16
-6	17	46
-6	5	58
		
		
// Clos, Mareike; Langner, Robert; Meyer, Martin; Oechslin, Mathias S.; Zilles, Karl; Eickhoff, Simon B. 2014, auditory, verbal, cue of different sentence>cue of same sentence		
//Subjects=29		
-52	18	18
		
// Ferstl et al 2005, auditory, verbal, incongruent>congruent		
//Subjects=20		
44	1	-19
		
// Gurd et al. 2002, auditory, verbal, switching>not switching		
//Subjects=11		
-15	-65	69
32	-60	66
		
//  Canini, Matteo; Della Rosa, Pasquale Anthony; Catricala, Eleonora; Strijkers, Kristof; Branzi, Francesca Martina; Costa, Albert; Abutalebi, Jubin, 2016, nonverbal, visual, time between exemplars of same category (interference)		
//Subjects=24		
-28	42	10
-14	6	18
		
//  Krieger-Redwood, Katya; Teige, Catarina; Davey, James; Hymers, Mark; Jefferies, Elizabeth, 2015, nonverbal, visual, strong>weak associations between pics		
//Subjects=22		
-30	50	4
4	34	28
2	34	32
0	20	48
32	50	8
-52	14	10
-4	-48	40
-6	-64	44
-4	-54	42
-12	-50	34
-12	-64	26
-4	-68	38
-56	-60	26
-56	-44	6
-34	-80	40
-62	-40	-4
-58	-40	-8
-62	-50	34
		
//  Schnur, TT; Schwartz, MF; Kimberg, DY; Hirshorn, E; Coslett, HB; Thompson-Schill, SL, 2009, nonverbal, visual, semantic blocks>mixed (i.e., more interference)		
//Subjects=16		
-48	21	24
-33	15	-9
-51	9	39
-45	-66	30
21	-9	3
60	-51	15
48	-54	18
		
//  Willems, RM; Ozyurek, A; Hagoort, P 2008, nonverbla, visual, chain pf pictures incongruent>congruent		
//Subjects=19		
-40	13	31
		
//  Sitnikova, Tatiana; Rosen, Bruce R.; Lord, Louis-David; West, W. Caroline, 2014, nonverbal ,visual, novel > typical object use		
//Subjects=16		
-42	0	45
-46	6	14
-31	24	40
		
//  Tobia, Michael J.; Madan, Christopher R., 2017, nonverbal, visual, subordinate >prototypical use		
//Subjects=16		
-47	-35	38
-25	-62	35
-42	12	28
-9	24	48
		
//  Thompson schill et al. 1997, nonverbal, visual, high>low selection		
//Subjects=6		
-3	18	44
-40	20	28
-40	13	33
46	21	31
-3	19	53
-52	-55	2
-40	-64	-14
-31	-71	41
		
//  Wig et al. 2009, nonverbal, visual, Reversed or different decision		
//Subjects=27		
-44	11	23
55	9	24
		
//  Abraham, 2018; VERBAL, VISUAL, Alternative uses task > name typical objects for location		
//Subjects=34		
-52	9	6
-45	22	11
-36	-8	0
-13	6	5
-13	-8	-7
16	-8	-7
-3	-17	7
-19	-31	-1
55	31	-2
-6	28	43
-3	20	54
-13	49	24
0	-1	36
-32	18	41
16	6	8
-58	-30	43
-48	-73	-3
-26	-105	-1
16	-8	-7
26	-78	-24
42	-66	-39
13	-90	14
-6	-93	15
		
//  Bitan, Tali; Kaftory, Asaf; Meiri-Leib, Adi; Eviatar, Zohar; Peleg, Orna, 2017; VERBAL, VISUAL, subordinate>dominant meaning relation		
//Subjects=23		
-46	20	28
-48	-32	-4
-40	4	28
-44	42	-6
		
//  Abraham, Anna; Pieritz, Karoline; Thybusch, Kristin; Rutter, Barbara; Kroeger, Soeren; Schweckendiek, Jan; Stark, Rudolf; Windmann, Sabine; Hermann, Christiane, 2012, VISUAL, VERBAL, USES>LOCATIONS 		
//Subjects=19		
-46	18	21
-50	22	1
-44	37	-14
-33	36	6
-53	10	-28
-1	24	23
-4	20	47
-36	10	39
-59	-32	40
-43	-25	49
-33	-10	-3
-50	-59	-8
34	-81	-34
18	-92	-16
25	-100	-5
		
//  Snyder, Hannah R.; Banich, Marie T.; Munakata, Yuko; 2011, visual, verbal, HIGH>LOW COMPETITION	plus	
//Subjects=18		
-6	16	60
-44	20	14
-30	26	-16
36	28	-12
10	56	12
-66	-28	-6
56	10	-36
16	-92	-8
-14	-92	-14
-42	20	14
-6	14	60
42	26	-24
-56	28	-6
16	-32	-16
-52	-52	-4
-68	-30	-4
-42	24	-26
30	-74	-30
-26	-58	-34
10	54	18
-6	14	60
4	4	64
-4	24	54
-2	38	26
10	34	22
-8	18	38
12	22	38
-50	24	18
-46	26	-4
-46	4	46
-44	10	38
-44	48	-12
-2	12	52
-4	48	48
-48	12	2
60	20	6
52	26	-6
40	40	-20
-26	46	24
30	52	32
50	6	44
44	50	-18
26	62	-12
44	26	42
20	-76	2
-16	-94	-12
-2	-96	-18
14	-88	-16
-26	-84	18
30	-84	10
-56	-48	-2
-66	-60	12
30	-76	14
-48	-66	-22
34	-56	16
52	-26	-10
4	-84	44
36	-52	-26
		
//  Balthasar, Andrea J. R.; Huber, Walter; Weis, Susanne; 2011; VERBAL, VISUAL, VISUAL HOMONYMS>IDENTIFICATION		
//Subjects=18		
-51	43	2
69	11	5
-33	-62	-6
53	48	-8
19	-36	55
14	46	9
45	6	56
6	-5	5
-16	-63	-14
-38	2	0
-3	7	36
-42	53	9
-58	-15	31
70	-14	34
28	6	56
-58	-19	29
57	-11	21
-34	7	-28
-38	2	4
-46	-66	27
10	-18	3
40	-68	9
10	50	42
-20	-58	-6
-66	-27	34
5	46	13
		
//  Hsu, Nina S.; Kraemer, David J. M.; Oliver, Robyn T.; Schlichting, Margaret L.; Thompson-Schill, Sharon L., 2011, VERBAL, VISUAL WITHIN COLOUR >BETWEEN COLOUR (I.E. DISTANCE TO FOIL)		
//Subjects=12		
-38	-28	-8
-31	55	51
56	40	13
-38	-50	-14
-15	-89	1
1	33	34
37	-45	45
-47	35	10
-41	16	31
63	44	-5
-43	-40	62
47	26	47
11	9	19
		
//  Sun, Jiangzhou; Chen, Qunlin; Zhang, Qinglin; Li, Yadan; Li, Haijiang; Wei, Dongtao; Yang, Wenjing; Qiu, Jiang; 2016; VERBAL, VISUAL, Alternative uses task > name typical objects for location		
//Subjects=28		
12	-87	12
		
//  Grindrod, CM; Bilenko, NY; Myers, EB; Blumstein, SE, 2008; VERBAL, VISUAL, ambiguity		
//Subjects=15		
1	-67	28
-49	32	-11
7	50	43
-58	-38	6
		
//  Jackson, Rebecca L.; Hoffman, Paul; Pobric, Gorana; Ralph, Matthew A. Lambon; 2015; visual, verbal, high>low association of foils		
//Subjects=24		
-51	15	27
-9	-96	-9
48	18	27
36	21	54
-39	-21	-24
30	24	-6
18	-93	-3
-30	-69	45
		
//  Madore, 2019; VERBAL, VISUAL, alternate uses> associations		
//Subjects=32		
-48	-31	36
-51	-62	-2
22	-68	-24
58	-58	-6
-38	-12	-2
0	5	36
60	-24	42
-26	-44	64
30	-54	70
27	-7	56
-50	5	24
-26	-10	62
32	-38	44
-20	-1	70
-44	-38	53
-26	32	-16
-6	-56	62
57	38	4
-8	-49	62
22	-67	-46
-34	-48	64
24	-48	68
-46	-40	-18
-58	-31	42
0	2	38
56	-66	-1
30	-74	-24
-52	-67	-1
-22	-2	68
46	-26	42
46	26	-10
33	-34	41
-39	-12	0
46	-43	-36
46	-79	17
-39	-7	10
32	-49	64
26	-6	66
-18	-73	-22
15	-74	-49
-33	38	-13
-44	2	22
9	-78	2
-48	-49	-20
-26	-2	-25
48	-37	-20
14	42	47
32	-12	-14
-12	-78	-48
18	-88	23
-28	-44	70
-15	6	68
-46	34	11
-44	-80	10
12	-74	-4
33	-2	-18
-9	-55	65
6	-84	30
52	38	6
		
//  Musz, Elizabeth; Thompson-Schill, Sharon L., 2017, VISUAL, VERBAL, subordinate>dominant homonym meaning		
//Subjects=13		
-49	40	5
-49	30	2
		
//  Hallam, Glyn P.; Whitney, Carin; Hymers, Mark; Gouws, Andre D.; Jefferies, Elizabeth, 2016, VERBAL, VISUAL, 		
//Subjects=18		
-52	24	20
-4	16	56
36	28	-8
12	-80	-32
44	24	24
12	12	48
-52	-40	0
20	-80	-48
		
//  Jeon, Hyeon-Ae, 2012, VERBAL, VISUAL, ambiguous homonym>unambiguous word		
//Subjects=14		
9	34	-14
47	-46	-10
-50	21	47
-25	32	-21
		
//  Mestres-Misse, Anna; Trampel, Robert; Turner, Robert; Kotz, Sonja A., 2016, VERBAL, VISUAL, ambiguous homonym>unambiguous word		
//Subjects=23		
35	24	-6
-42	21	-3
-50	35	13
-35	25	-5
-51	32	0
-51	13	19
		
//  Hoenig, K; Scheef, L, 2009, VERBAL, VISUAL, ambiguous homonym>unambiguous word		
//Subjects=22		
-27	7	-34
-5	54	33
-48	-61	45
-13	28	53
-7	-64	27
-37	17	44
23	47	40
60	-62	45
31	-17	-42
-1	46	-16
35	22	-37
-47	8	-49
-6	53	-1
-48	-83	34
30	27	64
		
//  Grindrod, Christopher M.; Garnett, Emily O.; Malyutina, Svetlana; den Ouden, Dirk B., 2014, ambiguous homonym>unambiguous		
//Subjects=23		
18	29	4
-54	38	4
18	29	4
-6	14	4
18	29	4
-18	20	-5
-51	14	-17
24	2	-5
-54	38	4
18	29	4
		
		
//  Hargreaves, Ian S.; Pexman, Penny M.; Pittman, Daniel J.; Goodyear, Bradley G., 2011, VERBAL, VISUAL, AMBIGUOUS>NOT		
//Subjects=20		
-40	8	24
-59	24	12
		
//  Satpute, Ajay B.; Badre, David; Ochsner, Kevin N., 2014, VERBAL, VISUAL, weakly related>strongly related target and focussing on task appropriate trait not general relatedness	2014	
//Subjects=33		
-39	27	18
51	33	24
9	15	0
-33	-21	57
12	-78	12
-18	-84	-9
		
//  Tylen, K.; Christensen, P.; Roepstorff, A.; Lund, T.; Ostergaard, S.; Donald, M., 2015, VERBAL, VISUAL, INCOHERENT>COHERENT STORY		
//Subjects=24		
4	28	34
34	48	28
52	-44	50
-56	-42	52
46	40	30
-32	42	28
58	-36	-16
36	16	0
-36	14	2
26	8	64
-16	6	64
-58	-58	-10
		
//  Li, Sai; Jiang, Xiaoming; Yu, Hongbo; Zhou, Xiaolin, 2014, VERBAL, VISUAL, incongruent>congruent sentences		
//Subjects=24		
4	44	18
2	40	34
		
//  Mestres-Misse, Anna; Bazin, Pierre-Louis; Trampel, Robert; Turner, Robert; Kotz, Sonja A., 2014, VERBAL, VISUAL, subordinacy/ambiguity/incongruuence		
//Subjects=23		
10	13	3
-45	26	18
-51	23	23
-48	23	21
-53	35	10
-49	27	6
-56	18	17
-43	23	-2
-29	24	-1
-52	32	0
30	24	-2
-14	3	12
44	29	14
51	26	23
-63	-39	-1
-60	-41	5
-49	30	9
-60	-37	8
		
//  Deen, Ben; McCarthy, Gregory, 2010, VERBAL, VISUAL, incongruent>congruent end of story		
//Subjects=15		
-2	24	20
40	10	-2
-30	22	-4
		
//  Newman, Sharlene D.; Ikuta, Toshikazu; Burns, Thomas, Jr., 2010, VISUAL, VERBAL, unrelated>related sentence		
//Subjects=20		
-28	18	-4
-44	24	4
-8	6	56
-44	-2	40
-26	-58	42
-2	-58	-20
-6	-16	-12
34	22	-2
42	22	24
30	-58	42
40	-16	62
		
//  Mano, Y; Harada, T; Sugiura, M; Saito, DN; Sadato, N, 2009, VISUAL, VERBAL, less coherent>more coherent texts		
//Subjects=18		
2	-68	26
46	-72	28
		
//  Peelle, JE; Troiani, V; Grossman, M, 2009, VERBAL, VISUAL, inconsistent>consistent description		
//Subjects=25		
-44	-66	30
-50	-18	-14
0	-40	40
44	-68	34
60	-2	-18
-32	58	6
0	48	-6
-48	30	18
-48	38	-14
30	58	2
-10	-26	-24
4	-54	20
4	-16	14
30	10	44
38	14	48
		
//  Zhu, Zude; Hagoort, Peter; Zhang, John X.; Feng, Gangyi; Chen, Hsuan-Chih; Bastiaansen, Marcel; Wang, Suiping, 2012, VERBAL, VISUAL, incongruence/cloze probability parameter		
//Subjects=27		
-2	20	54
-48	26	24
50	20	30
-56	-48	-4
-30	-60	58
32	-66	56
-42	40	34
-2	22	54
32	56	12
-54	-46	40
-42	48	-2
-18	-84	-26
32	-62	46
10	-84	-24
		
//  Carter, Benjamin, T., Foster, Brent, Muncy, Nathan M., Luke, Steven G., 2019; VERBAL, VISUAL, surprisal of word in context		
//Subjects=41		
2	44	-5
2	-59	62
62	-44	26
32	-86	32
-41	-17	5
59	-65	14
-2	-74	53
-2	53	-5
53	-71	-29
-26	20	44
		
//  Huang, Jian; Zhu, Zude; Zhang, John X.; Wu, Mingxiang; Chen, Hsuan-Chih; Wang, Suiping, 2012, VERBAL, VISUAL, unexpected>expected word		
//Subjects=23		
-36	26	-4
0	24	50
-52	-80	4
-58	-39	30
35	42	-3
		
//  Kambara, Toshimune; Tsukiura, Takashi; Yokoyama, Satoru; Takahashi, Kei; Shigemune, Yayoi; Miyamoto, Tadao; Takahashi, Daiko; Sato, Shigeru; Kawashima, Ryuta, 2013, VERBAL, VISUAL, semantic violations>normal sentences		
//Subjects=38		
-38	30	-23
-18	63	12
41	48	-22
33	-5	-9
		
//  Nieuwland, Mante S.; Martin, Andrea E.; Carreiras, Manuel, 2012, VERBAL, VISUAL, semantic anomalies>normal sentences		
//Subjects=24		
-50	34	-16
-36	26	-24
-32	20	-20
46	38	-18
44	28	-14
28	16	-18
-4	42	44
-6	48	22
6	38	56
		
//  Ye, Zheng; Donamayor, Nuria; Muente, Thomas F., 2014, VERBAL, VISUAL, incongruent>congruent sentence		
//Subjects=20		
6	16	58
-44	26	-2
56	19	1
		
//  van de Meerendonk, Nan; Rueschemeyer, Shirley-Ann; Kolk, Herman H. J., 2013, VERBAL, VISUAL, LESS CONGRUENT>MORE CONGRUENT WORD		
//Subjects=24		
18	4	18
38	20	22
36	36	8
-40	40	-2
-40	40	-14
-52	32	10
42	-8	40
52	-4	44
54	-8	22
-28	-8	40
-46	-12	44
-36	-10	44
12	-44	-38
20	-40	-36
		
//  van de Meerendonk, Nan; Rueschemeyer, Shirley-Ann; Kolk, Herman H. J., 2013, VERBAL, VISUAL, incongruent>congruent word		
//Subjects=24		
38	20	20
50	30	10
32	28	-12
50	0	-22
42	-2	-22
50	8	-30
		
//  Willems, RM; Ozyurek, A; Hagoort, P, 2008, VERBAL, VISUAL, words sentences incongruent>congruent		
//Subjects=19		
-47	20	21
-55	-35	-1
		
//  Raposo, Ana; Mendes, Mafalda; Marques, J. Frederico, 2012, VERBAL, VISUAL, rarer>prototypical feature		
//Subjects=17		
-60	20	12
		
//  Raposo, Ana; Mendes, Mafalda; Marques, J. Frederico, 2012, VERBAL, VISUAL, rarer>prototypical feature OF BASIC ITEM		
//Subjects=17		
-54	24	8
4	30	10
-44	24	-12
		
//  Kroeger, Soeren; Rutter, Barbara; Stark, Rudolf; Windmann, Sabine; Hermann, Christiane; Abraham, Anna, 2012, VERBAL, VISUAL, appropriate and unusual use >inappropriate and usual uses	2012	
//Subjects=19		
-36	35	4
-45	11	16
51	38	4
27	29	-11
-27	23	-14
51	26	1
-9	23	46
-21	14	52
-9	26	22
-9	29	28
-6	32	31
9	-13	-8
-9	-5	-1
-6	-10	-8
		
//  Rutter, Barbara; Kroeger, Soeren; Stark, Rudolf; Schweckendiek, Jan; Windmann, Sabine; Hermann, Christiane; Abraham, Anna, 2012, VERBAL, VISUAL, 		
//Subjects=18		
-48	17	4
60	17	4
-42	20	1
-36	29	-8
-33	50	13
-30	53	19
-48	20	-14
		
//  Zhu, Zude; Feng, Gangyi; Zhang, John X.; Li, Guochao; Li, Hong; Wang, Suiping, 2013, VERBAL, VISUAL, violation>low cloze>high cloze parametric modulator		
//Subjects=26		
-40	20	24
-64	-46	-4
-46	-10	-20
-4	22	52
-46	22	34
-8	-70	38
-36	-58	42
0	26	38
46	26	30
24	50	-12
4	-26	26
		
//  Moberget, Torgeir; Gullesen, Eva Hilland; Andersson, Stein; Ivry, Richard B.; Endestad, Tor, 2014, VERBAL, VISUAL, incongruent>congruent sentence		
//Subjects=32		
-10	-84	-31
18	-74	-29
-10	-84	-29
18	-74	-29
9	29	41
-54	-40	8
		
//  Van Ettinger-Veenstra, Helene; McAllister, Anita; Lundberg, Peter; Karlsson, Thomas; Engstrom, Maria, 2016, VERBAL, VISUAL, INCONGRUENT>CONGRUENT SENTENCE		
//Subjects=27		
-50	28	6
52	26	24
		
//  Allen et al., 2008, VERBAL, VISUAL, Inappropriate > appropriate		
//Subjects=15		
-44	30	30
-42	38	32
8	-70	50
-4	-70	54
-24	32	-18
		
//  Badre et al.,2005, VERBAL, VISUAL, Weak > Strong association		
//Subjects=22		
-51	27	-3
-48	30	-12
-45	9	51
-51	9	33
-39	12	48
-42	12	18
-48	15	24
-51	21	21
-9	21	42
-45	27	15
-33	27	-6
-48	30	12
-36	30	-3
-45	42	-9
33	0	57
42	12	27
6	21	51
9	30	12
36	30	-9
-39	3	27
-42	9	21
-54	12	18
-21	15	60
-48	18	18
-30	27	-18
-54	30	12
-3	33	48
-45	39	3
30	24	-15
		
//  Bedny et al, 2008, VERBAL, VISUAL,  ambiguous >not		
//Subjects=20		
-54	9	39
-3	33	42
36	-69	-39
-24	57	24
-12	-6	18
-45	-57	54
-57	-42	-3
0	27	42
39	-66	-39
-48	15	9
-51	6	45
		
//  Bunge 2005, VERBAL, VISUAL, Low > High probe target RELATEDNESS		
//Subjects=20		
-57	24	6
	 	
//  Chan et al., 2004, VERBAL, VISUAL, Semantic ambiguity > non-ambiguous		
//Subjects=8		
-37	59	26
-10	68	18
-47	48	21
-39	-17	67
24	2	64
49	34	15
12	67	19
6	39	-22
37	-17	46
49	-59	45
-14	-92	18
-52	-74	-5
14	-93	45
10	-75	14
7	-82	-6
7	22	-17
6	-30	26
		
//  de Zubicaray et al. 2000, VERBAL, VISUAL, Inappropriate > appropriate aka response suppression> response initiation		
//Subjects=8		
1	51	-17
2	36	39
4	47	-24
-17	-56	48
17	-74	44
-2	-69	12
-2	-71	-12
36	-74	-18
-12	44	33
-33	-54	48
17	-79	32
-2	-75	7
1	-79	1
-2	-71	-12
		
//  Gennari et al, 2007, VERBAL, VISUAL, Ambiguous > unambiguous		
//Subjects=17		
-52	-61	5
-44	21	-4
-35	-43	48
-50	4	13
		
//  Hirshorn  and  Thompson Schill, 2006, VERBAL, VISUAL, Switching > Free generation		
//Subjects=10		
-42	3	66
-18	48	-6
-3	-57	63
-57	-57	54
-3	-66	60
-3	-72	0
48	24	54
48	24	42
48	42	30
3	51	-15
39	57	3
36	42	-9
12	-27	-27
42	-51	45
-39	24	18
		
//  Hirshorn  and  Thompson Schill, 2006, VERBAL, VISUAL, switch>cluster		
//Subjects=10		
-12	21	63
-30	54	18
-51	45	9
-54	21	39
-33	18	51
-39	-3	51
15	54	-3
-33	27	6
-39	-24	33
-33	-63	21
-18	-21	15
-39	-39	36
-39	-66	39
-12	-66	51
39	45	39
36	51	15
15	33	48
66	-33	21
51	-30	-18
36	-54	42
48	-60	-33
		
//  Ketteler et al., 2008, VERBAL, VISUAL, weakly > strongly related targets and stronger > weaker distractor relation		
//Subjects=12		
-12	48	38
21	45	16
-48	39	-27
53	19	4
43	39	-28
50	-51	43
-52	-37	58
-31	-16	-16
-21	41	49
-2	-15	10
-47	36	22
-66	-43	-6
50	33	18
-2	-39	2
41	-60	44
-2	-29	6
		
//  Nagel et al, 2008, VERBAL, VISUAL, High > low selection		
//Subjects=14		
-51	26	-4
-46	-39	46
-44	36	18
		
//  Nelson et al 2009, VERBAL, VISUAL, Many > Few associates		
//Subjects=17		
-8	18	40
0	11	50
-45	0	50
-52	23	20
-56	11	20
		
//  Noppeney et al, 2004,  VERBAL, VISUAL, Different > Same triads/easy judgements		
//Subjects=15		
-44	32	10
37	-46	-28
-44	-64	-18
-41	-22	-15
-32	20	-13
-5	32	46
-9	-5	-1
-44	14	25
1	-64	-49
		
//  Persson et al. 2004,  VERBAL, VISUAL, HIGH>LOW SELECTION		
//Subjects=22		
-49	26	15
41	15	5
-52	-52	-5
-4	8	60
		
//  Race et al. 2009, VERBAL, DIFFERENT>SAME ATTRIBUTE/reversed/novel decision > repeated		
//Subjects=26		
-51	36	12
-48	33	0
-45	15	24
-51	12	15
-24	0	66
-39	-6	54
-12	18	30
-42	33	-3
-45	42	-9
-45	9	21
-54	-42	3
-45	-63	-24
-39	-48	-27
-33	-33	-21
-33	-33	-30
-45	-57	-12
-45	-51	-15
		
//  Roskies et al., 2001, VERBAL, VISUAL, Hard > Easy category verification based on typicality of category		
//Subjects=20		
-54	23	-8
17	-92	-25
		
//  Snyder et al., 2007, VERBAL, VISUAL, Specific attribute related> globally sem related		
//Subjects=14		
-24	-102	-12
-51	-57	-18
-57	-48	-3
-27	-69	48
-39	-45	51
33	-48	51
-54	27	24
-36	42	0
-57	15	6
54	27	24
-60	9	39
42	18	48
0	45	57
		
//  Spalek et al. 2008, semantically related > unrelated distractors		
//Subjects=21		
-35	-47	-16
17	-86	-25
		
//  Thompson schill et al. 1997, VERBAL, VISUaL, High > low selection (man>few poss responses)	/similar colour vs generally similar	
//Subjects=6		
-52	13	29
-47	9	29
42	20	18
-43	34	2
-3	45	29
-3	19	53
-52	-59	-7
-35	-66	53
		
//  Thompson schill et al., 1999, VERBAL, VISAUL, Different attribute > Same Attribute		
//Subjects=8		
-46	20	19
		
//  Wagner et al., 2001, verbal, visual, Weak > Strong association	/ more foils>less	
//Subjects=14		
-45	27	-12
-51	18	27
-51	21	-3
6	18	39
9	27	36
0	9	57
45	21	6
30	24	-6
39	27	-9
54	24	27
-36	21	27
-39	6	24
-45	27	9
3	30	36
3	15	42
-51	21	-12
		
//  Whitney et al. 2009, verbal, visual, Ambiguity		
//Subjects=18		
-44	20	28
-48	24	32
-48	28	16
-44	12	24
-52	32	-4
-52	20	24
-48	8	40
		
//  Zhang et al. 2004, verbal, visual, High conflict > Low conflict (reversible>non-reversible words)/neutral		
//Subjects=14		
-25	23	-21
39	20	-22
6	37	17
-41	23	-14
-45	15	18
1	25	33
39	20	-18
		
		
// Collette et al 2001, verbal visual, inhibit>initiate		
//Subjects=12		
-33	21	27
-40	48	-13
-55	24	14
-46	25	18
-33	26	29
-46	27	18
-55	27	23
-57	36	22
-42	44	12
-49	45	3
-45	50	-13
-38	56	2
-36	63	-6
-32	70	0
42	72	-15
-47	27	-4
-53	38	-3
-38	28	-16
-53	33	-9
-47	45	-17
66	39	-21
-30	55	-32
		
		
// Mason & Just 2007, verbal visual, ambiguous>unambiguous		
//Subjects=12		
-28	12	16
24	16	12
26	46	16
-8	18	6
12	18	8
-20	54	18
34	30	6
24	46	16
-16	46	14
-52	26	12
24	12	10
-12	20	12
-24	22	2
-22	-28	0
46	18	10
-12	-12	10
-42	32	10
-46	24	2
-8	4	8
-56	26	16
-16	8	14
		
// Zempleni et al., 2007, verbal visual, ambiguous>unambiguous		
//Subjects=16		
-48	26	20
-52	16	26
34	20	-10
-50	-48	-12
56	-34	-16
		
// Liu et al 2009, verbal visual, weak>strong		
//Subjects=16		
-58	26	4
-54	20	32
		
		
// Liu et al 2009, verbal auditory, weak>strong		
//Subjects=16		
-58	20	0
-52	18	28

//Addis et al., 2011; conjunction of autobiographical tasks (covert generation of autobiographical memory based on written cue)> imagery/semantic decision tasks		
//Subjects=15		
6	53	10
-56	-20	-7
57	1	-17
-55	-19	45
4	-43	30
-48	-60	34
51	-61	31
40	-83	15
6	6	2
				
//Addis et al., 2007; event elaboration (autobiographical memory covert elaboration of past/future event with written cue) > retrieve two words related to the cued word then generate a sentence/think of 2 objects related to the stimulus and visualize all 3 objects in a triangular arrangement
//Subjects=14		
-1.23	66.26	-3.63
-5.32	53.47	29.46
24.35	38.64	41.4
-3.3	42.04	-19.72
-3.3	48.89	-11.31
-3.16	9.88	-10.86
3.09	35.35	-9.31
-2.95	-11.23	40.53
-22.09	-19.11	-16.53
-17.77	-36.26	-4.28
30.83	-35.85	-4.59
-34.94	13.52	-33.12
-54.89	-10.07	-19.17
71.9	-7.82	-17.85
-11.29	-39.67	42.53
3.47	-50.49	18.77
-7.06	-51.09	27.75
-11.25	-52.48	40.08
-47.17	-55.02	38.24
50.04	-52.75	44.17
-25.96	-98.82	8.93
-3	-56.09	-37.42
11.78	-54	-39.87
			
//Addis et al., 2012; event elaboration (autobiographical memory covert elaboration of past/future event with written cue) > retrieve two words related to the cued word then generate a sentence/think of 2 objects related to the stimulus and visualize all 3 objects in a triangular arrangement	
//Subjects=15		
32	32	-24
-56	12	-32
-20	-84	-16
-40	12	52
8	12	64
-20	8	-24
-24	-28	-16
-16	-12	-20
-48	-76	28
-32	-96	4
-20	40	36
52	-68	24
-36	-52	-8
4	-60	12
-8	60	-12
20	-12	-20
-32	-16	-24
-36	-88	-40
			
//Audrain et al., 2022; overtly generate the recalled autobiographical event after choosing from 2 pictures > overtly describe the picture		
//Subjects=40		
2	-65	-1
-1	54	15
34	-81	-4
-24	-87	-7
-24	-40	-17
28	-38	-17
8	-79	-21
-20	-17	-18
-40	-69	-18
-24	24	51
-1	56	-13
-43	-72	34
-17	63	10
8	-52	-39
59	-6	-19
-56	-3	-16
-43	19	27
-1	18	48
43	-69	33
24	-18	-21
-1	-28	-17
-36	38	-12
-14	-95	33
43	-43	45
-59	-40	42
50	42	2
				
//Compere et al., 2016; men: covertly recall autobiographical event > sentence completion and visualize the sentence they completed		
//Subjects=16		
-12	-54	12
-6	-54	30
-9	-60	21
-30	15	-12
				
//Compere et al., 2016; women: covertly recall autobiographical event > sentence completion and visualize the sentence they completed		
//Subjects=20		
-6	-57	24
5	54	15
3	57	6
-45	-69	30
-39	-69	45
-39	-51	27
39	-63	36
48	-55	36
54	-57	21
36	30	24
-18	30	36
				
//Denkova et al., 2015; overtly generating autobiographical response (written cue) > semantic verbal fluency 		
//Subjects=18		
-45.3	18.37	47.63
59.14	30.13	0.71
-49.78	35.15	-22.3
-45.5	17.97	-27.81
47.44	31.76	-22.64
43.24	28.84	-17.98
39.05	10.07	-31.09
-7.24	24.95	63.61
-7.48	61.66	25.58
0.89	57.39	-10.8
5.32	42.89	44.55
-61.08	-36.29	1.53
-57.03	-10.63	-28
-52.83	-2.19	-28.61
63.42	-5.22	-25.75
22.39	-40.26	-6.46
-19.92	-28.01	-7.05
22.32	-24.78	-11.96
-28.56	6.35	-30.44
-49.28	-57.27	36.19
43.69	-56.49	35.59
9.93	-83.14	18.74
9.87	-72.12	9.11
9.84	-43.97	38.26
-7	-79.35	14.14
-7.03	-52.44	40.05
1.31	-42.64	9.36
			
//Ford et al., 2011; autobiographical memory search upon musical cue > select an adjective that describe the music and give a definition to the adjective selected 		
//Subjects=18		
-10	-60	26
8	-58	20
-28	-38	-18
28	-38	-18
-62	-20	4
56	2	-10
-58	-6	-8
-64	-14	-8
54	-20	-8
58	-6	-12
-8	0	-12
6	-2	-18
4	8	-16
-2	58	-10
-12	44	-14
-14	36	-18
18	-24	-16
18	-18	-28
-2	28	-8
24	-30	24
-22	-18	30
				
//Gardini et al., 2005; EAM >baseline		
//Subjects=14		
-9.23	-51.2	8.9
-3.08	-5.33	-0.94
-5.08	-38.19	-3.11
-5.27	33.04	23.09
-0.99	18.3	22.96
-0.91	14.54	47.64
-34.89	15.85	-12.19
-30.65	19.58	-3.59
-28.6	26.01	-19.58
-17.79	-42.33	-17.18
-28.37	-38.21	-17.4
-22.29	61.06	17.94
-30.73	56.65	16.07
-18.14	68.35	-1.45
5.42	21.16	51.58
-0.91	14.47	46.53
-5.27	34.09	23.01
5.4	-12.48	1.72
5.44	-16.14	10.85
-3.02	-16.35	8.7
				
//Gilmore et al., 2021; autobiographical recall > picture description		
//Subjects=46		
-1	-20	47
62	-31	28
-43	2	-3
50	-6	-12
43	-71	2
2	56	-6
-20	-58	5
-59	1	-16
2	29	44
24	-56	-9
47	-9	47
18	-85	24
-43	-71	2
-56	-35	21
2	-60	26
-43	-9	50
			
//Gurguryan & Sheldon, 2019; retrieval orientation (conceptual and contextual) > number detection task		
//Subjects=28		
42	-76	32
-18	-58	16
-38	-78	32
18	-54	16
-32	-38	-14
24	12	48
-10	-62	58
30	-34	-14
-10	-50	-54
-22	6	52
38	-38	40
-54	-60	-2
-32	-42	40
//Gurguryan & Sheldon, 2019; stage of retrieval (re-oriented and oriented) > number detection task		
//Subjects=28		
12	-90	18
-16	12	0
-28	-66	40
-40	6	36
20	16	10
-46	-76	0
34	-66	-50
10	-80	-38
-6	6	64
-6	-72	-24
-62	-40	0
			
//Holland et al., 2011; covertly generate autobiographical memory from written cue > sentence generation with given format (x is smaller than y is smaller than z)		
//Subjects=31		
-7.59	63.49	-13.39
24.58	-53.29	4.41
63.72	-55.17	19.84
-63.36	-5.94	-19.41
57.04	2.87	-30.71
-54.57	-61.99	28.77
-2.77	-67.72	31.08
28.98	-81.18	30.71
-40.87	-20.37	69.13
				
//Ino et al., 2011; overtly generate autobiographical memory > rest and autobiographical memory > semantic memory		
//Subjects=21		
-7.07	-56.95	18.16
-4.96	-38.77	39.1
7.7	-55.13	11.29
11.88	-47.1	4.06
41.36	-60.74	-50.68
-38.89	6.29	56.19
32.88	21.05	45.88
-3.36	54.51	-22.79
-38.77	-18.25	68.97
-42.89	-68.5	41.36
-60.08	-24.99	-3.69
-51.85	20.09	-26.81
-22.35	66.48	3.14
			
//Iriye & St Jacques 2020; negative EAM time lag 3		
//Subjects=25		
-12	60	6
-8	50	18
-14	26	42
62	26	14
-56	22	20
-42	24	-8
40	18	18
-40	30	6
-66	-46	14
-50	-20	-6
52	-10	26
-52	-66	18
40	-28	20
-42	-86	12
34	-82	6
22	-98	16
//Iriye & St Jacques 2020; negative EAM time lag 4		
//Subjects=25		
-24	10	-22
4	32	-14
-16	50	8
28	32	54
-40	20	-10
54	32	-6
40	32	-6
-38	20	18
28	32	54
42	2	60
40	-82	-8
42	-18	-4
64	-2	-18
-52	-26	-8
4	-50	24
-54	-64	18
-32	-66	-14
-22	-84	16
-22	-88	-12
-6	14	-10
-4	-40	-6
//Iriye & St Jacques 2020; negative EAM time lag 5		
//Subjects=25		
-26	40	22
56	34	18
-52	28	20
-42	24	-18
-32	30	2
24	28	-14
-46	20	32
-48	-8	42
8	-34	64
60	-2	10
-26	-26	66
32	6	-20
-60	-6	4
50	16	-26
50	-14	-8
-60	-10	-16
58	-38	6
-2	46	2
-4	-10	44
-2	12	-8
6	-50	24
-8	-32	36
-44	-68	36
-66	-50	10
-58	-30	24
50	-74	10
-34	-94	6
22	-96	20
12	-62	-8
-26	-68	-12
-16	6	4
28	-4	14
//Iriye & St Jacques 2020; negative EAM time lag 6		
//Subjects=25		
4	44	44
10	18	38
-4	64	0
-22	42	24
56	26	12
-40	22	-20
52	2	4
-26	-6	-22
-36	0	-6
-62	-14	-14
60	-36	2
66	-4	-16
0	18	24
-4	26	34
4	-48	24
38	-26	20
-54	-62	8
48	-70	10
-28	-92	-2
36	-92	4
-4	-12	6
6	-22	10
-14	18	10
				
//Martinelli et al., 2013; young: overt autobiographical memory retrieval > overt sentence completion		
//Subjects=20		
-9	-39	36
-9	-54	30
6	57	3
-51	-69	27
-42	-72	39
-57	-60	21
36	-66	42
42	-51	39
54	-60	30
			
//Martinelli et al., 2013; old: overt autobiographical memory retrieval > overt sentence completion		
//Subjects=17		
9	-45	-3
-12	-39	3
-12	-48	9
-18	-9	12
-18	3	30
-15	27	18
48	-66	39
51	-60	21
45	-54	27
-18	-18	-12
9	-27	-9
-48	-60	30
-48	-60	21
-9	51	18
	
//McCormick et al., 2015; EAM > math task (lag 1)		
//Subjects=18		
48	-2	-22
-58	-2	-22
-60	26	-2
-46	8	-38
40	8	-30
//McCormick et al., 2015; EAM > math task (lag 2)		
//Subjects=18		
54	-2	-18
54	-66	30
-10	52	24
-66	-38	-2
-42	26	-8
-40	10	46
2	50	-16
-10	-42	34
-10	-88	0
26	-34	-16
-30	-34	-18
20	-86	24
-14	-94	16
//McCormick et al., 2015; EAM > math task (lag 3)		
//Subjects=18		
58	-4	-20
52	-58	24
-8	-56	10
-50	-58	16
-2	38	16
16	-78	-6
52	30	-12
//McCormick et al., 2015; EAM > math task (lag 4)		
//Subjects=18		
-16	40	42
-8	-46	34
52	-60	26
-58	-2	-22
56	-4	-20
24	-86	26
14	-78	-6
40	28	-20
-8	-96	12
2	26	18
14	-88	-40
//McCormick et al., 2015; EAM > math task (lag 5)		
//Subjects=18		
52	-68	38
-6	-46	32
-44	-66	30
6	52	-16
-46	4	-32
58	-2	-20
-40	18	36
36	-12	64
24	-78	-34
-6	36	16
-32	0	-18
//McCormick et al., 2015; EAM > math task (lag 6)		
//Subjects=18		
-50	10	-32
2	-50	14
6	44	27
-44	-68	30
40	8	-30
40	-12	62
-44	16	50
54	-68	34
-26	-88	20
6	-52	-44
40	-14	4
46	-16	16
10	-98	6
//McCormick et al., 2015; EAM > math task (lag 7)		
//Subjects=18		
-2	-54	18
-12	64	26
-44	-68	30
-52	8	-34
52	-70	32
40	-12	62
-8	-56	-50
-4	34	10
-50	22	-18
-10	18	66
		
		
//Sheldon & Moscovitch, 2011; overtly generate words after reading written autobiographical cue > visuomotor baseline task		
//Subjects=20		
-9.24	-53.71	2.41
5.44	-58.67	-46.17
-30.16	-72.97	52.69
-60	-43.77	-0.19
-30.48	-31.32	-7.87
-9.25	-49.51	2.13
14	-51.28	4.33
			
//Sperduti et al., 2013; covertly generating descriptions that describe the autobiographical memory after written cue > rest condition	
//Subjects=20		
-6	15	45
-39	9	54
-36	-27	54
-6	18	63
-24	27	51
-51	12	36
-18	36	45
-54	24	15
-9	54	36
-9	60	-6
-33	48	12
0	-36	33
-33	-18	63
-36	45	3
30	27	0
-6	-57	18
6	-57	-48
-36	-72	42
-48	-69	30
-39	-33	48
-30	-69	51
-45	-27	51
-27	-36	-18
30	-57	-27
-54	-39	-12
-21	-18	-15
24	-18	-21
-9	-60	12
12	-54	0
9	-72	-30
-39	-87	-6
-30	-90	-6
39	-75	-18
0	-66	-30
-30	24	-3
		
//St Jacques et al., 2012; activation during overt generation of autobiographical memory > rest	
//Subjects=17		
4	56	23
-49	34	15
-56	26	4
56	30	-4
-38	0	57
-38	23	46
49	8	-38
4	-56	38
-53	-53	30
56	-56	23
-26	19	49
23	26	42
0	53	-15
-23	-15	-19
-30	-38	-11
30	-38	-15
-8	-64	15
0	-41	34
-45	-79	23
53	-68	19
0	34	38
-41	49	-4
-49	23	30
-4	-26	27
-53	-45	49
41	-60	46
-11	0	11
-8	-30	-30
8	34	27
-26	49	23
34	41	30
-38	15	8
34	15	4
41	-4	49
-4	-75	53
				
//Summerfield et al., 2009; EAM (real + imagined) > filmes and new events (real + imagined)	
//Subjects=18		
-12	54	33
-27	51	27
-18	39	33
-33	36	-12
-6	33	-12
6	33	24
3	18	24
-24	6	60
-51	9	33
-36	-24	39
18	6	21
-9	-15	12
6	-30	36
-6	-54	6
-9	-72	54
-39	-81	27
-6	-96	0
18	-84	-36

//D'Argembeau et al., 2014; EAM remembering > EAM reasoning		
//Subjects=24		
8	-48	4
-28	-44	-16
30	-42	-14
6	-54	40
46	-64	20
-56	-56	-8
52	-14	-18
-38	-80	28
-24	10	58
34	6	54
24	-8	-18
-28	0	-20
4	58	-12
//D'Argembeau et al., 2014; EAM reasoning > EAM remembering		
//Subjects=24		
-10	34	56
-8	62	28
-50	20	10
-44	38	-14
-6	18	64
-56	-34	-4
-48	-62	30

//Ford et al., 2016; All EAM > control search (young adults)		
//Subjects=16		
-16	28	40
-6	-10	54
-10	50	46
46	22	10
54	6	36
8	8	34
-64	-2	12
58	-14	26
-48	20	28
-40	-68	36
-40	-34	34
36	-66	38
54	-18	48
-68	-26	-14
-10	4	0
-12	-54	8
30	-22	-2
36	-30	-24
6	-72	-30

//Monge et al., 2018; autobiographical memory > laboratory memory recovery		
//Subjects=18		
-12	-52	10
3	57	-5
-50	-63	17
22	-14	-21
22	-30	-17
-23	-41	-13
-20	23	40

//Rabin & Rosenbaum, 2012; EAM > pTOM (TR1,2)		
//Subjects=18		
-12	20	36
52	44	8
-20	28	8
24	-68	-48
//Rabin & Rosenbaum, 2012; EAM > pTOM (TR4,5)		
//Subjects=18		
16	-56	20

//Arshamian et al., 2013; odor cued EAM > control odors		
//Subjects=15		
39	-72	18
-33	-75	15
36	-39	39
-30	-42	-39
//Arshamian et al., 2013; word cued EAM > control words		
//Subjects=15		
18	51	3
57	-12	6
39	-69	-3
-27	3	-17
24	-27	-30
21	-75	3
0	-96	3
-33	-72	12
-42	-27	15
24	-18	3
21	-45	-24
51	-51	-33

//Chen et al., 2017; life memory > picture memory		
//Subjects=31		
-20	31	49
-8	12	67
22	27	53
-9	-55	12
14	-49	14
62	-6	-19
-60	-4	-17
48	-68	26
-45	-67	25
-11	60	29
-2	57	7
29	-12	-17
-23	-27	-15
-1	39	-2
15	40	14
-8	-58	49
-4	-41	39
-36	5	57
51	10	-28
12	-26	67
-11	-31	67
-8	12	67
-24	-2	-12
-59	-13	-1
40	-61	9
-46	34	4
-47	18	25
6	-54	-46
42	-61	-33
25	-81	-33

//Detour et al., 2011; presented personal photographs mixed with photographs of unknown people: discrimination (remember/know/new) trial > observation trial		
//Subjects=8		
-46	26	-14
2	36	52
44	-68	48
-50	28	22
-62	-44	-12
-46	-60	30
44	24	46
62	-36	-12
46	46	-14

//Donix et al, 2010; EAM > semantic memory (younger subjects)		
//Subjects=15		
48	-48	20
-44	-46	10
48	-26	-4
-50	-24	-10
50	18	-10
-38	18	-12
48	12	-26
-50	4	-28
4	-8	10
-8	-10	0
-4	-56	44
-12	-46	36
8	6	62
-4	12	62
-42	8	36
-26	54	24
26	-26	-6

//Eich et al., 2009; first person perspective memory retrieval (field perspective) main effect		
//Subjects=16		
-12	24	52
-48	36	-4
-48	28	4
12	56	28
-52	-12	-24
-44	-8	-36
-60	-44	-8
12	-52	32
-44	-64	28
-52	-56	48
-56	-64	36
52	-68	36
48	-76	32
56	-60	40
0	-44	-24
44	-68	-36
28	-84	-36
20	-88	-36
4	-52	-48
-36	-48	0
-16	-20	-24

//Fleischer et al., 2019; EAM > arithmetic (placebo condition)		
//Subjects=33		
4	28	-2
-50	-68	38
-4	-54	28
-48	30	-6
-46	8	-34
54	-64	34
-62	-12	-14
-20	-16	-20

//Hoscheidt et al., 2010; episodic spatial > ungrammatical sentence reading		
//Subjects=17		
42	-74	40
-46	-78	30
10	46	28
18	-2	-16
-22	-2	-18
8	-44	0
-22	-84	-28
12	8	18
-6	6	6
-12	-58	20
58	24	30
-54	30	18
2	40	44
56	26	32
-40	12	34
14	52	46
22	-36	-16
-32	-36	-20
20	-22	-14
-16	-28	-12
28	24	-12
10	-46	4
-6	-48	2
4	-58	30
44	-66	24
-44	-78	32
-34	-72	40
20	-22	-16
-26	-28	-24
10	-46	6
-8	-54	12
8	24	52
44	-66	22
-48	-72	22
8	-8	10
8	-46	2
//Hoscheidt et al., 2010; episodic nonspatial > ungrammatical sentence reading		
//Subjects=17		
36	-64	40
-48	-76	26
8	-50	28
4	-62	20
-2	-64	22
46	24	-12
-30	32	-14
56	24	26
38	10	58
-42	12	34
14	48	34
12	46	36
16	-4	-16
-18	-20	-16
28	22	-14
-30	18	-14
10	-42	2
-6	-46	2
18	-2	-18
-14	-30	-12
4	-60	20
-4	-56	18
4	-12	10
-8	-6	8
-2	-42	0
		
//Lux et al., 2013; EAM with personalized sentence cues: main effect of temporal and locational context		
//Subjects=13		
2	-46	38
4	52	-10
22	-26	-10
-16	-32	-8
-4	-54	-36
	
//Maguire and Frith, 2003; EAM > word recognition control task		
//Subjects=12		
-3	54	-3
-48	9	-30
48	18	-33
-28	-13	-18
21	-18	-15
-54	-6	-24
-21	-39	-15
-6	-63	24
-45	-63	21
30	-81	-36
54	36	0

//Noreen et al., 2016; EAM > EAM suppression (major and minor clusters)		
//Subjects=22		
-21	-79	-11
0	-88	-5
-6	-103	10
21	-94	28
9	-46	22
-6	-49	22
-39	-67	28
6	-58	16
0	-97	1
-54	26	13
-45	2	46
-3	17	61
-33	14	58
-60	-46	-2
24	-70	-11
-18	-28	-8
-51	-19	-20
-36	-52	46
-6	-70	40
21	-58	28
-27	-61	46
30	-73	-20
24	-82	-17
-18	-55	-14
	
//Oddo et al., 2010; EAM > semantic memory		
//Subjects=15		
-20	-20	-14
-32	22	-16
8	54	0
8	-56	24
28	80	40
22	82	40
	
//St Jacques et al., 2013; main effect of memory condition: hit > false alarm		
//Subjects=35		
-10	52	36
-14	-10	-14
38	-32	-16
	
//St Jacques et al., 2018; EAM > counterfactual simulation		
//Subjects=29		
-10	28	-10
-16	44	-6
-10	32	0
-64	-30	38
-60	-28	24
54	-28	32
66	-24	22
	
//Bisby et al., 2016; main effect of associative memory		
//Subjects=20		
33	-72	33
-24	-66	36
-21	-18	-15
-48	30	18
51	33	15
-30	-84	-6
51	-57	-3
-48	-42	54
	
//Ford et al., 2010; word pair: unrelated and compound word (intact > recombined)		
//Subjects=18		
-56	-50	40
8	-24	44
-4	34	34
-4	-46	32
-38	26	52
-38	14	-10
60	-22	30
	
//Giovanello et al., 2004; associative > item recognition		
//Subjects=16		
-45	9	-36
54	-6	-36
24	-3	-21
27	24	-18
51	-12	-18
54	0	-18
-12	-6	-15
-57	-3	-15
42	36	-12
-6	51	-12
63	-33	-6
-54	-51	-3
-54	30	-3
-3	-15	15
-51	-36	24
-54	30	3
-3	-15	15
-51	-36	24
-12	-66	30
-45	-75	30
-39	-57	33
-6	-27	45
	
//Hannula et al., 2009; correct > incorrect recognition of scene-face pair		
//Subjects=12		
-21	0	-36
-48	42	0
-45	33	6
-48	27	30
-45	9	-42
-36	6	-45
57	-69	27
45	-69	36
45	-78	36
33	-54	48
-15	9	-6
	
//Holdstock et al., 2010; face-laugh/face-face recognition > perceptual baseline		
//Subjects=12		
38	-70	-14
14	-90	-8
14	-94	-4
32	-92	4
26	-94	2
38	-50	-16
68	-32	-10
34	20	34
16	-76	-24
-20	-88	-12
-16	-90	-10
-4	-94	-6
-28	-94	8
-12	-98	-2
-48	-66	-14
-36	-72	-20
38	-56	46
32	-70	40
40	-58	50
36	-56	34
44	-22	52
-40	-46	40
-12	-64	36
-32	-66	52
44	32	24
44	22	-4
32	24	2
14	6	18
10	-12	10
-4	-16	12
-44	36	16
-38	26	0
-42	2	36
-28	20	-2
4	14	54
4	28	46
4	24	40
-2	12	56
-2	34	38
-10	28	58
2	-34	30
-4	-34	28
-34	-74	-42
	
//Huijbers et al., 2013; face-name pairs: hit (categorize intact pairs as old ones)> missed (categorized intact pairs as rearranged/new one) recognition		
//Subjects=45		
-12	-67	29
12	-70	26
-42	-61	44
42	-55	50
-39	44	8
-6	17	50
-6	32	35
-33	2	62
-30	26	-10
33	-29	-4
-9	14	-4
12	11	2
-12	-79	2
		
//Lepage et al., 2003; intact+rearranged pairs > new pairs		
//Subjects=6		
-6	42	27
-39	36	-6
-54	24	21
-6	18	48
-30	15	45
-36	-54	51
-12	-72	48
33	24	-15
18	-60	27
12	-78	33
	
//Li et al., 2016; object pair: contextual > noncontextual		
//Subjects=16		
-26	-6	-20
-34	-10	-22
36	-6	-24
34	-14	-26
26	-53	-6
20	-64	-4
4	-46	62
6	-46	70
-14	56	12
-16	54	4
12	56	6
20	-20	4
//Li et al., 2016; shape pair: contextual > noncontextual		
//Subjects=16		
-34	-10	-22
-32	-18	-22
-24	-8	-18
36	-16	-22
34	-14	-24
34	-4	-22
16	-66	-4
30	-48	-8
12	-42	-4
6	-46	58
14	-40	46
-14	56	12
-2	60	6
12	60	6
-12	-32	0
-20	-22	-2
10	-12	18
	
//Okada et al., 2011; adjective + face pair: recognition > word 		
//Subjects=15		
-34	18	-6
44	-54	-28
-38	4	38
-10	-82	-14
0	-60	-44
-8	12	50
-32	-62	38
-24	-80	12
-26	-92	4
8	-74	-50
-46	16	-16
-34	-88	-10
20	-96	-10
-24	-30	-6
	
//Onoda et al., 2009; word pair (neutral) > baseline		
//Subjects=25		
-40	10	26
2	30	42
-28	-64	44
32	-58	42
-12	-94	-4
22	-96	-2
-22	-30	-4
//Onoda et al., 2009; word pair (negative) > baseline		
//Subjects=25		
-48	18	28
2	30	42
-50	-44	-2
-26	-62	44
36	-62	44
-20	-64	-10
22	-94	-2
	
//Park et al., 2014; object pair: associative recognition (hit > miss)		
//Subjects=19		
-51	29	-2
54	29	-2
39	-25	64
-42	-1	16
-6	41	25
0	-16	28
-48	-70	22
57	-22	16
-54	-76	7
-9	-49	-2
-15	-85	4
3	-55	-44
	
//Sherman et al., 2015; successful associative memory retrieval (correct memory for intact vs control task)		
//Subjects=41		
-6	-50	10
-52	32	-6
-40	-74	42
-12	14	4
30	66	-10
-46	-62	26
-14	-4	-18

//Daselaar et al., 2008; access > elaboration (young adults)		 	 
// Subjects=17
25.78	-15.65	-16.92
5.86	-53.88	23.06
41.78	30.99	-24.05
54.27	25.72	22.2
9.82	68.21	19.87
38.56	23.88	60.75
5.05	47.34	-19.44
5.93	19.97	43.76
-47.71	-62.04	-26.77
//Daselaar et al., 2008; elaboration > access (young adults)		 
// Subjects=17
-39.71	-85.04	2.26
33.7	-85.2	-3.45
-26.03	-47.92	65.59
-47.09	27.88	39.4
-44.56	47.78	-11.92
-56.1	18.56	9.1
-51.15	-51.15	43.93
43.1	-44.49	64.08
-43.04	-24.46	7.53
65.98	-13.85	0.14

//Rabin et al., 2009; EAM > TOM construction
// Subjects=18		 
35.45	-14.97	-21.64
-30.42	-23.71	-19.65
24.74	-20.87	-15.27
-26.99	-45.21	-8.62
-25.02	-.74	-15.29
24.08	55.58	33.21
47.81	8.37	20.67
34.28	35.84	-17.68
24.99	-63.86	-4.3
-7.29	-65.16	6.47
//Rabin et al., 2009; EAM > TOM elaboration		 
// Subjects=18
21.51	-25.14	-14.79
-27.3	-26.76	-29.49
-5.61	48.34	-8.15
-5.53	56.61	-0.01
-17.32	59.42	7.76
5.58	66.59	25.71
50.84	46.15	11.27
0.37	-56.76	15.59
-.43	-74.35	34.15

//Speer et al., 2014; positive EAM > neutral EAM
// Subjects=19		 
48.4	-64.24	-32.67
57.93	-3.27	-35.5
22.57	-79.97	-27.31
-32.56	-60.71	-24.92
12.82	-37.59	-21.25
-16.18	-82.37	-16.33
41.82	17.06	-23.8
-3.61	29.66	-24.27
-61.77	-15.21	-18.84
64.62	-1.69	-18.97
16.08	-21.01	-16.23
13.15	-90.9	-5.9
-10.16	71.74	-21.6
-42.44	36.53	-17.56
6.39	-7.65	-10.66
-3.29	2.51	-4.78
-51.71	-35.64	3.18
13	3.19	1.6
-6.43	-0.07	2.25
19.93	-101.77	15.23
-3.02	-11.89	13.45
-2.74	-49.24	27.23
-12.91	65.59	16.04
-9.6	46.46	17.87
-54.87	20.77	21.19
-35.2	6.13	35.75
0.58	17.07	47.5
//Speer et al., 2014; positive EAM > neutral EAM		 
// Subjects=19
62.28	-34.38	44.82
	 
//Addis et al., 2004a; specific EAM > general EAM (lag 2)
// Subjects=14		 
-33.22	48.58	30.39
-7.2	-50.3	16.2
-41.29	-66.45	52
-15.69	-87.45	21.14
//Addis et al., 2004a; specific EAM > general EAM (lag 3)		 
// Subjects=14
-3.81	64.24	-33.29
26.72	-18.92	-28.94
-15.74	-52.79	24.44
-45.78	-63.35	39.44
53.64	-58.43	40.62
22.68	-93.39	-21.51
-50.61	12.72	15.19
56.28	15.87	8.57
-7.02	12.38	43.61
-41.6	3.44	41.73
36.31	-24.9	44.32
-20.55	-24.77	-8.51
-62.22	-23.62	29.07
65.43	-38.29	37.31
14.61	-73.85	27.13
-50.62	-65.08	-1.77
36.27	-73.36	31.19
	 
//Addis et al., 2004b; specific and general AM retrieval > sentence completion and size discrimination		 
// Subjects=14
-3.54	54.21	-14.37
-7.01	-36.83	33.92
5.99	-32.12	37.71
3.39	-11.32	7.68
-24.97	-23.28	-15.3
37.75	-27	-12.64
-24.89	-36.9	-11.72
18.21	-10.26	-16.21
24.8	-33.53	-12.9
-62.9	-7.87	-20.66
-9.28	-52.98	22.1
7.97	-43.34	20.86
-4.77	-55.86	36.88
-41.32	-73.24	48.19
-47.92	-64.32	40.69
-6.83	-47.94	-38.95

//Botzung et al., 2008a; EAM > semantic memory		 
// Subjects=10
-6.87	-54.39	41.25
-2.74	-57.96	25.85
-15.83	-61.17	15.18
-52.67	25.45	25.17
-57.14	33.11	15.52
-52.54	28.61	36.06
-26.37	32.7	55.38
-43.85	15.88	37.17
-2.94	45.6	31.29
5.94	27.73	46.36
-2.77	22.82	40.27
2.97	54.44	-12.26
-1.3	61.43	-6.16
5.27	62.07	0.39
-61.6	18.88	1.32
-48.96	20.89	-23.75
-55.19	22.74	-3.65
-50.11	-72.19	37.03
-48.08	-60.04	29.07
-50.33	-65.13	20.65
-34.87	-86.61	42.68
-39.13	-79.62	48.78
-61.89	-11.68	-27.03
-66.07	-25.97	-18.82
64.36	-59.02	33.77
51.52	-75.67	40.12
47.16	-67.17	39.36
59.88	-62.41	20.74
44.16	25.01	-7.81
50.59	31.21	-10.78
67.91	-36.34	-22.32
16.28	8.93	5.46
7.54	3.82	-2.85
-20.71	-28.14	-21.62
		 
//Botzung et al., 2008b; past events evocation main effect		 
// Subjects=10
-2.87	-54.54	16.55
-45.81	-65.79	36.32
-56.51	-56.28	46.77
-53.49	8.37	-42.6
-61.79	-21.9	-21.54
45.01	-75.9	38.02
0.86	54.84	-7.79
-20.25	45.33	29.37
-45.98	29.26	42.61
-39.97	56.36	10.69
3.66	-14.77	28.19
	 
//Cabeza et al., 2004; Conjunction: EAM triggered by photos taken by self and EAM triggered by photos taken by others		 
// Subjects=13
54.09	32.02	9.25
37.58	31.91	-13.99
-35.87	28.46	-12.39
-39.37	-84.68	29.12
47	-82.66	22.96
-2.74	24.09	42.38
-2.93	-85.67	5.06
13.14	-48.28	2.21
-23.81	-28.18	-10.36
25.9	-23.54	-9.42

//Denkova et al., 2006; EAM (relatives and friends' faces)> control (face identification; yes for familiar faces and no for unknown faces)		 
// Subjects=10
18.66	-55.08	9.51
-4.47	-72.28	56.43
-30.62	13.54	57.35
53.64	-71.61	37.44
-34.92	-89.15	38.45
32	17.81	53.62
-43.6	-47.93	43.48
-44.06	33.99	24.18
22.5	-34.58	-23.96
13.71	18.99	-24.63
11.75	24.89	-7.25
-12.05	11.42	-12.23
-9.83	18.44	-6.24
-22.84	-43.25	-22.33
-12.05	-25.78	-19.76
-31.46	-36.48	-18.37
-3.41	48.66	-4.86
5.29	42.51	-2.16
-31.4	53.64	4.09
-35.95	14.76	-21.12
-43.93	13.13	30.72
10.39	17.73	53.99
14.54	25.22	41.98
16.66	35.86	40.89
5.28	-29.76	-17.42
7.6	-26.38	-4.35

//Gilboa et al., 2004; main effect of vividly reexperienced events		 
// Subjects=9
-28.27	-22.64	-19.79
-2.62	-55.96	35.73
-19.36	38.46	13.23
-35.95	-64.72	47.25
-48.33	-18.12	18.2
-65.6	-3.83	22.68
2.94	-62.45	46.37
-9.26	-28.75	28.67
-47.26	-55.42	9.55
-58.4	-8.74	-7.21
64.1	-50.05	14.96
30.04	0.31	-18.58
-13.95	-86.2	-11.51
10.94	-77.08	-7.23
13.65	-83.87	34.86
10.96	10.6	12.11
28.18	0.36	4.98
-28.68	-76.03	-62.69
		 
//Grol et al., 2017; imagery of memory from field perspective > visual search (numbers)		 
// Subjects=27
-25.39	29.75	46.69
-39.27	62.28	-17.92
-22.85	70.48	0.04
-48.92	36.51	-17.45
-.28	53.26	-13.21
-3.47	17.52	-16.35
-52.33	2.41	-37.55
-45.7	6.87	-24.66
-47.85	-73.68	43.86
-31.72	-21.64	48.52
-2.64	-41.93	36.59
-19.14	-44.17	13.56
-29.19	-36.81	-10.53
23.1	28.98	35.86
26.75	69.93	-8.6
58.31	32.37	1.3
58.19	-4.59	-15.21
55.72	-71.06	31.75
26.29	-46.88	16.41
25.72	-11.71	-20.67
35.44	-90.54	-37.69

//Muscatell et al., 2010; EAM > sentence reading		 
// Subjects=13
3.63	40.82	37.26
2.79	59.68	-25.11
-20.93	68.72	-18.87
28.92	65.68	-8.22
-18.2	66.4	25.01
-28.81	33.91	34.01
29.63	37.07	41.67
-11.67	2.22	15.56
-63.85	-23.31	-13.52
-60.79	-8.28	-25.14
-4.72	-47.9	41.7
-41.35	-53.79	49.63
55.8	-69.37	38.31
	 
//Niki & Luo, 2002; recall the paces where participants visited 7 years ago or within 2 years when they were presented with landmarks of the places (remote > recent memories)		 
// Subjects=9
-16.51	43.21	-16.42
-5.91	52.64	-30.98
20.36	7.18	-13.49
20.4	19.5	-7.98
31.02	22.55	-21.91
67.24	13.75	19.8
-11.44	38.95	41.06
55.9	-31.45	53.6
43.09	-50.01	62.38
40.89	-39.39	61.37
14.23	60.13	25.08
14.63	30.3	50.44
16.48	53.11	30.22
58.31	-40.96	-13.85
-39.93	34.78	9.46
14.84	11.03	62.42
-45.77	-.96	52.32
69.73	-38.48	35.01
-22.04	-44.87	40.57
-11.65	55.99	28.17
	 
//Rekkas and Constable, 2005; retrieval of remote EAM main effect		 
// Subjects=12
-17.08	55.77	26.05
-12.42	46.26	50.44
34.79	56.05	26.25
25.61	22.87	62.19
-20.89	23.46	59.56
31.96	9.84	48.8
-2.59	23	53.69
5.52	51.93	18.2
-.42	29.26	-28.77
-9.56	13.92	14.37
-59.45	-16.19	-6.45
55.04	-15.99	-10.66
-22.86	-13.8	-17.4
24.69	-12.47	-17.22
-21.75	-25.6	-17.37
24.75	-31.72	-16.44
11.24	-47.48	22.33
-8.21	-47.55	22.67
-47.01	-60.14	27.94
42.73	-59.3	31.93
9.27	-47.54	-47.1
-8.02	-47.6	-46.8
40.62	-62.63	-49.51
-39.33	-62.92	-48.12
	 
//Svoboda and Levine, 2009; EAM rehearsal main effect		 
// Subjects=11
-17.19	62.59	18.65
9.94	48.31	25.2
-7.27	53.13	31.74
-45.2	20.39	16.58
-42.64	9.17	45.66
21.95	23.04	29.74
-12.61	-55.84	14.6
10.16	-49.61	22.56
-36.65	-34.94	-1.63
-32.49	-17.57	-11.26
-57.59	5.66	-25.46
-43.01	-32.97	8.37
40.17	-27.25	7.51
33.73	-26.96	10.95
-30.35	-30.85	-15.58
19.41	-36.74	-12.49
60.43	-2.9	-8.69
-35.22	-62.95	20.17
-5.19	-74.39	-.5
-1.08	-6.95	-2.76
14.1	1.17	2.9
-9.57	13.82	13.26
12.05	18.36	14.68
-7.92	-49.11	-39.93
-31.56	-34.02	-26.46
-11.14	-70.81	-42.21
27.85	-29.45	-26.8
	 
//Young et al., 2013; specific AM recall > example generation (male + female)		 
// Subjects=40
-52.08	33.83	-11.53
62.79	23.75	12.16
-42.6	16.07	50.58
0.3	61.98	34.1
-58.48	-9.36	-13.87
62.35	5.12	-26.33
27.92	-9.27	-17.59
-24	-7.74	-21.34
-15.48	-13	-32.17
25.8	-35.21	-19.47
-57.83	-59.22	26.91
54.48	-54.77	22.32
0.68	-66.48	37.84
-30.18	-12.06	1.6
34.65	-7.37	2.28
32.56	-92.21	-9.46
-27.3	-82.56	-40.77
30.03	-77.48	-35.53
	 
//Achim & Lepage, 2005a; associative recognition > item recognition		 
// Subjects=18
-46.23	27.39	22.63
-32.65	-51.11	54.81
32.18	-59.6	52.31
		 
//Achim & Lepage, 2005b; associative recognition: intact > rearranged
// Subjects=18		 
-25.19	-16.22	-31.69
-57.29	-68.69	-16.98
12.73	-42.24	55.4
//Achim & Lepage, 2005b; associative recognition: rearranged > intact		 
// Subjects=18
-44.57	38.96	-14.41
-33.62	31.32	-4.87
-41.89	31.87	24.35
-39.15	-16.27	60.44
-38.96	-19.28	74.18
44.54	54.07	27.41
16.89	41.98	60.45
33.91	22.54	35.19
48.9	6.25	20.86
	 
//Bader et al., 2014; general recognition (same > new)		 
// Subjects=20
-3.8	-44.68	30.16
-42.51	-54.42	42.98
55.75	-58.84	36.15
-43.61	-67.7	38.71
	 
//Burgess et al., 2001; person-object association main effect
// Subjects=12		 
-4.69	-71.71	39.57
-40.48	19.06	48.01
-43.95	33.75	33.16
-34.68	63.2	3.2
-28.01	58.09	17.04
-18.01	43.73	35.1
26.72	65.36	-11.51
-51.04	25.87	-16.36
-31.67	22.12	-23.04
-36.95	-77.28	50.75
53.75	-54.31	50.3
-1.82	46.98	34.5
-1.73	-31.16	25.42
//Burgess et al., 2001; person-place association main effect		 
// Subjects=12
37.4	-81.76	33.12
-11.58	-55.09	11.15
35.91	-55.23	6.99
8.53	-59.92	61.71
-10.96	-70.18	56.33
23.75	-37.69	-11.35
33.47	-47.53	-13.9
17.01	-16.62	-26.76
-31.46	-42.01	-20.06
-15.23	-38.45	-17.33
33.22	49.12	-13.37
33.53	35.04	8.18
-8.89	32.71	-14.39
-11.82	41.57	11.67
37.54	-74.13	45.81
21.16	-29.52	41.68
4.89	-23.51	38
-8.05	-29.94	38.85
	 
//Cabeza et al., 1997; PAL recognition > reading		 
// Subjects=12
24.72	24.94	-7.47
26.98	27.91	1.16
26.83	37.71	-8.77
-2.75	25.16	42.28
-35.03	-65.96	33.91
40.62	-58.9	36.41
		 
//Dennis et al., 2014; hit > correct reject (face-scene pairs)		 
// Subjects=18
-47.91	38.22	-22.12
-50.09	-7.46	51.91
34.36	-34.3	58.73
59.09	-9.96	53.66
-5.46	10.05	-4.37
-34.69	-16.72	-13.55
75.07	-37.81	30.37
-62.07	-44.37	36.72
		 
//King et al., 2005; person-object association main effect		 
// Subjects=13
29.99	58.99	-10.93
-31.66	54.96	-16.21
-2.5	35.3	-21.48
-51	36.06	-10.65
-14.67	-50.66	24.21
-33.78	-81.08	44.35
-47	-69.92	26.66
17.32	15.21	3.7
-18.16	3.26	15.57
39.79	26.81	-11.28
-25.1	22.77	-16.5
-56.82	-44.43	24.31
-57.27	-41.16	-9.62
-21.71	-38.47	-17.22
-21.38	-22.9	11.49
1.25	-19.94	7.45
17.14	-94.44	-32.51
36.51	-85.11	-37.13
36.57	-68.23	-28.71